{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412705db",
   "metadata": {},
   "source": [
    "LangGraph Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.exceptions import ApiException\n",
    "\n",
    "os.environ[\"NRP_API_KEY\"] = \"Api key here\"\n",
    "config.load_incluster_config()\n",
    "\n",
    "v1 = client.CoreV1Api()\n",
    "apps_v1 = client.AppsV1Api()\n",
    "batch_v1 = client.BatchV1Api()\n",
    "networking_v1 = client.NetworkingV1Api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6748b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a433d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"NRP_API_KEY\"),\n",
    "    base_url=\"https://llm.nrp-nautilus.io/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47da1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_pods(namespace=\"gsoc\"):\n",
    "    \"\"\"\n",
    "    Describe pods and print only fields useful for Prometheus metric queries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pods = v1.list_namespaced_pod(namespace=namespace) if namespace else v1.list_pod_for_all_namespaces()\n",
    "\n",
    "        rows = []\n",
    "        for pod in pods.items:\n",
    "            pod_name = pod.metadata.name\n",
    "            ns = pod.metadata.namespace\n",
    "            pod_ip = pod.status.pod_ip\n",
    "            node = pod.spec.node_name\n",
    "            container_names = [c.name for c in pod.spec.containers]\n",
    "            container = \", \".join(container_names)\n",
    "\n",
    "            rows.append([pod_name, ns, pod_ip, node, container])\n",
    "\n",
    "        headers = [\"Pod\", \"Namespace\", \"Pod IP\", \"Node\", \"Container\"]\n",
    "        print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"❌ Error fetching pods: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a21f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════════════════════════╤═════════════╤════════════════╤════════════════════════════╤═════════════╕\n",
      "│ Pod                              │ Namespace   │ Pod IP         │ Node                       │ Container   │\n",
      "╞══════════════════════════════════╪═════════════╪════════════════╪════════════════════════════╪═════════════╡\n",
      "│ agno-deployment-55c55964db-lzhkx │ gsoc        │ 10.244.215.212 │ hcc-nrp-shor-c6013.unl.edu │ jupyter     │\n",
      "├──────────────────────────────────┼─────────────┼────────────────┼────────────────────────────┼─────────────┤\n",
      "│ my-postgres-cluster-0            │ gsoc        │ 10.244.91.149  │ k8s-gen4-02.ampath.net     │ postgres    │\n",
      "├──────────────────────────────────┼─────────────┼────────────────┼────────────────────────────┼─────────────┤\n",
      "│ shellshock-cluster-0             │ gsoc        │ 10.244.19.231  │ dtn-gpu2.kreonet.net       │ postgres    │\n",
      "╘══════════════════════════════════╧═════════════╧════════════════╧════════════════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "describe_pods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c6d7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#namespace gpu utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4be1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def namespace_gpu_utilization(prom_url=\"https://prometheus.nrp-nautilus.io\", threshold=0):\n",
    "    \"\"\"\n",
    "    Display average GPU utilization per namespace using PromQL.\n",
    "    Args:\n",
    "        prom_url (str): Base Prometheus URL.\n",
    "        threshold (float): Minimum % utilization to show (filtering).\n",
    "    \"\"\"\n",
    "    query = 'avg by (namespace) (DCGM_FI_DEV_GPU_UTIL)'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            print(\"❌ Prometheus query failed.\")\n",
    "            return\n",
    "\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            print(\"✅ Query successful, but no GPU usage data returned.\")\n",
    "            return\n",
    "\n",
    "        rows = []\n",
    "        for r in results:\n",
    "            ns = r[\"metric\"].get(\"namespace\", \"unknown\")\n",
    "            util = float(r[\"value\"][1])\n",
    "            if util >= threshold:\n",
    "                status = (\n",
    "                    \"🟢 Low\" if util < 40 else\n",
    "                    \"🟡 Moderate\" if util < 70 else\n",
    "                    \"🔴 High\"\n",
    "                )\n",
    "                rows.append([ns, f\"{util:.2f}%\", status])\n",
    "\n",
    "        headers = [\"Namespace\", \"Avg GPU Utilization\", \"Status\"]\n",
    "        print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying Prometheus: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66825ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════════════════════════╤═══════════════════════╤═════════════╕\n",
      "│ Namespace                          │ Avg GPU Utilization   │ Status      │\n",
      "╞════════════════════════════════════╪═══════════════════════╪═════════════╡\n",
      "│ gpu-mon                            │ 0.15%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csusb-hpc                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nrp-llm                            │ 7.08%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csusb-xli                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-goldberg                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ spatiotemporal-decision-making     │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-rci-jh                        │ 7.69%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-shen-climate-lab              │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsb-cms-ml                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ jupyterlab                         │ 18.48%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsc-llm                           │ 49.88%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-vigir-gpu               │ 13.29%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ rse-kube                           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ espm-157                           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ehf                                │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ coder                              │ 23.62%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ razvanlab                          │ 21.88%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-llm                           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aiea-auditors                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ chei-ml                            │ 59.60%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-aicenter                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ seelab                             │ 47.06%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nsf-reu                            │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csusb-chaseci                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ lemn-lab                           │ 19.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ omniverse                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ksu-nrp-cluster                    │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ cms-ml                             │ 45.42%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ system-test                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-sknnh                   │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ hjepa                              │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aiea-slugbotics                    │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ wenglab-interpretable-ai           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ecepxie                            │ 35.20%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aiea-interns                       │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aoi-lab-scratch                    │ 90.33%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ai-tutoring                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ librareome                         │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ svcl-srip25-clip                   │ 19.75%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsd-ravigroup                     │ 89.75%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ cogrob                             │ 19.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-mizzou-xu                │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nourish-sdsc                       │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csun-deep-learning                 │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ urcs-vista                         │ 75.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ genai-lab                          │ 92.86%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ chronic-opioid-lab                 │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ anthony-lab                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gilpin-lab                         │ 39.50%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ai-schmidt                         │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csu-tide-jupyterhub                │ 30.67%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-unoselab02               │ 23.75%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-smile                         │ 59.75%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ engr131spring                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ environmental-analytics-group-usra │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ jkb-lab                            │ 98.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nsf-maica                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ isaac-sim                          │ 51.00%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-mizzou-hpdi-pretrain     │ 85.12%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ niddk                              │ 99.44%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsd-haosulab                      │ 39.48%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ evl                                │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ rohanm                             │ 93.50%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ assel                              │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ai-fusion-ga                       │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csuf-it-test                       │ 90.88%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ svcl-srip25-ad                     │ 83.80%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ erl-ucsd                           │ 29.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ dataset-distillation               │ 63.00%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-bml                     │ 30.50%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ svcl-clip                          │ 43.75%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ kc-ai-research-lab                 │ 98.88%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-hpc                     │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ cal-poly-humboldt-microglia        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ amnh-astro-jfagin                  │ 70.75%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csuf-research                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ z-lab                              │ 100.00%               │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gsoc                               │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ williamli-scvl                     │ 19.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-malof                    │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ece-tarajavidi                     │ 33.33%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ osg-ligo                           │ 11.77%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsb-opus-lab                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ vlsida                             │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ pa-riemann                         │ 75.50%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ osg-icecube                        │ 58.67%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ continual-open-world-learning-lab  │ 66.50%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ mizzou                             │ 100.00%               │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-sgs                     │ 93.67%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ amnh-herpetology-jhoffman1         │ 78.50%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ shanxiaojun                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ dl4nlpspace                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ compression                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gai-lina-group                     │ 74.50%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nicest-ychen                       │ 0.00%                 │ 🟢 Low      │\n",
      "╘════════════════════════════════════╧═══════════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "namespace_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b8d4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════════════════╤═══════╤════════════════════════════╤═══════════════╤═════════════╤════════════════════════════════════════════════════════╕\n",
      "│ Host                          │   GPU │ Model                      │ Utilization   │ Namespace   │ Pod                                                    │\n",
      "╞═══════════════════════════════╪═══════╪════════════════════════════╪═══════════════╪═════════════╪════════════════════════════════════════════════════════╡\n",
      "│ k8s-gpu-03.sdsc.optiputer.net │     7 │ NVIDIA GeForce GTX 1080 Ti │ 0.00%         │ gpu-mon     │ dcgm-export-dcgm-exporter-r5cz4                        │\n",
      "├───────────────────────────────┼───────┼────────────────────────────┼───────────────┼─────────────┼────────────────────────────────────────────────────────┤\n",
      "│ gpu-01.csusb.edu              │     0 │ NVIDIA RTX A5000           │ 0.00%         │ csusb-hpc   │ jupyter-anthony-2elopez6979-40coyote-2ecsusb-2eedu     │\n",
      "├───────────────────────────────┼───────┼────────────────────────────┼───────────────┼─────────────┼────────────────────────────────────────────────────────┤\n",
      "│ gpu-01.csusb.edu              │     1 │ NVIDIA RTX A5000           │ 0.00%         │ csusb-hpc   │ jupyter-anthony-2elopez6979-40coyote-2ecsusb-2eedu     │\n",
      "├───────────────────────────────┼───────┼────────────────────────────┼───────────────┼─────────────┼────────────────────────────────────────────────────────┤\n",
      "│ gpu-01.csusb.edu              │     2 │ NVIDIA RTX A5000           │ 0.00%         │ csusb-hpc   │ jupyter-justin-2estrickland7157-40coyote-2ecsusb-2eedu │\n",
      "├───────────────────────────────┼───────┼────────────────────────────┼───────────────┼─────────────┼────────────────────────────────────────────────────────┤\n",
      "│ gpu-01.csusb.edu              │     3 │ NVIDIA RTX A5000           │ 0.00%         │ csusb-hpc   │ jupyter-justin-2estrickland7157-40coyote-2ecsusb-2eedu │\n",
      "╘═══════════════════════════════╧═══════╧════════════════════════════╧═══════════════╧═════════════╧════════════════════════════════════════════════════════╛\n",
      "\n",
      "🔍 Total GPUs: 1327\n",
      "📊 Average Utilization: 25.74%\n",
      "🔴 Fully Utilized GPUs (>=99%): 200\n",
      "🟢 Idle GPUs (<1%): 914\n",
      "💻 Unique Host Machines: 226\n",
      "🧠 Unique GPU Models: 25\n",
      "🧮 GPUs Available (<100%): 1183\n",
      "\n",
      "📈 Top 10 GPUs by Utilization:\n",
      "| Host                         |   GPU | Model                 | Utilization   | Namespace   | Pod                                           |\n",
      "|------------------------------|-------|-----------------------|---------------|-------------|-----------------------------------------------|\n",
      "| node-2-10.sdsc.optiputer.net |     0 | NVIDIA A100 80GB PCIe | 100.00%       | sdsc-llm    | llama3-3-vllm-inference-5f98bd9744-jx97r      |\n",
      "| node-2-10.sdsc.optiputer.net |     1 | NVIDIA A100 80GB PCIe | 100.00%       | sdsc-llm    | llama3-3-vllm-inference-5f98bd9744-jx97r      |\n",
      "| node-2-10.sdsc.optiputer.net |     2 | NVIDIA A100 80GB PCIe | 100.00%       | sdsc-llm    | llama3-3-vllm-inference-5f98bd9744-jx97r      |\n",
      "| gpu00.nrp.hpc.udel.edu       |     0 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu00.nrp.hpc.udel.edu       |     1 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu00.nrp.hpc.udel.edu       |     2 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu00.nrp.hpc.udel.edu       |     3 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| discover-nrp-01.sdccd.edu    |     1 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n",
      "| discover-nrp-01.sdccd.edu    |     6 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n",
      "| discover-nrp-01.sdccd.edu    |     7 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tabulate import tabulate\n",
    "\n",
    "def fetch_dcgm_gpu_util_data(prom_url=\"https://prometheus.nrp-nautilus.io\"):\n",
    "    \"\"\"\n",
    "    Fetch rich GPU utilization data from Prometheus using DCGM_FI_DEV_GPU_UTIL.\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts with context: [{hostname, gpu_id, model, namespace, pod, utilization, ...}]\n",
    "    \"\"\"\n",
    "    query = 'DCGM_FI_DEV_GPU_UTIL'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            print(\"❌ Prometheus query failed.\")\n",
    "            return []\n",
    "\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            print(\"✅ Query successful, but no GPU data returned.\")\n",
    "            return []\n",
    "\n",
    "        enriched = []\n",
    "        for r in results:\n",
    "            m = r[\"metric\"]\n",
    "            val = float(r[\"value\"][1])\n",
    "            enriched.append({\n",
    "                \"hostname\": m.get(\"Hostname\", \"unknown\"),\n",
    "                \"ip_port\": m.get(\"instance\", \"unknown\"),\n",
    "                \"gpu_id\": m.get(\"gpu\", \"N/A\"),\n",
    "                \"device\": m.get(\"device\", \"N/A\"),\n",
    "                \"uuid\": m.get(\"UUID\", \"N/A\"),\n",
    "                \"model\": m.get(\"modelName\", \"unknown\"),\n",
    "                \"namespace\": m.get(\"namespace\", \"N/A\"),\n",
    "                \"pod\": m.get(\"pod\", \"N/A\"),\n",
    "                \"utilization\": val\n",
    "            })\n",
    "\n",
    "        return enriched\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying Prometheus: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def display_gpu_data_head(data, n=5):\n",
    "    \"\"\"\n",
    "    Display the first `n` GPU entries with rich context.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    rows = [\n",
    "        [d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]]\n",
    "        for d in data[:n]\n",
    "    ]\n",
    "    print(tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "def analyze_dcgm_gpu_data(data):\n",
    "    \"\"\"\n",
    "    Analyze DCGM GPU data with statistics and top utilization.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to analyze.\")\n",
    "        return\n",
    "\n",
    "    total = len(data)\n",
    "    avg_util = sum(d[\"utilization\"] for d in data) / total\n",
    "    maxed = [d for d in data if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in data if d[\"utilization\"] < 1.0]\n",
    "    available = [d for d in data if d[\"utilization\"] < 100.0]\n",
    "    unique_hosts = set(d[\"hostname\"] for d in data)\n",
    "    unique_models = set(d[\"model\"] for d in data)\n",
    "\n",
    "    print(f\"\\n🔍 Total GPUs: {total}\")\n",
    "    print(f\"📊 Average Utilization: {avg_util:.2f}%\")\n",
    "    print(f\"🔴 Fully Utilized GPUs (>=99%): {len(maxed)}\")\n",
    "    print(f\"🟢 Idle GPUs (<1%): {len(idle)}\")\n",
    "    print(f\"💻 Unique Host Machines: {len(unique_hosts)}\")\n",
    "    print(f\"🧠 Unique GPU Models: {len(unique_models)}\")\n",
    "    print(f\"🧮 GPUs Available (<100%): {len(available)}\\n\")\n",
    "\n",
    "    print(\"📈 Top 10 GPUs by Utilization:\")\n",
    "    top = sorted(data, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [[d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]] for d in top]\n",
    "    print(tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"github\"))\n",
    "\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    display_gpu_data_head(data, n=5)\n",
    "    analyze_dcgm_gpu_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a04c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from typing import Optional\n",
    "from io import StringIO\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Helper to capture and truncate printed output\n",
    "def capture_stdout_truncated(func, max_length=2000, *args, **kwargs):\n",
    "    \"\"\"Capture stdout and truncate if too long to prevent LLM loops\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    try:\n",
    "        func(*args, **kwargs)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    output = mystdout.getvalue()\n",
    "    if len(output) > max_length:\n",
    "        output = output[:max_length] + f\"\\n\\n... [Output truncated - showing first {max_length} characters]\"\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30355e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools with truncated outputs\n",
    "@tool\n",
    "def describe_pods_tool(namespace: Optional[str] = \"gsoc\") -> str:\n",
    "    \"\"\"Describe pods in a given Kubernetes namespace. Defaults to 'gsoc'.\"\"\"\n",
    "    return capture_stdout_truncated(describe_pods, 1500, namespace=namespace)\n",
    "\n",
    "@tool\n",
    "def namespace_gpu_util_tool(threshold: Optional[float] = 0.0) -> str:\n",
    "    \"\"\"Get average GPU utilization per namespace with optional threshold filter.\"\"\"\n",
    "    return capture_stdout_truncated(namespace_gpu_utilization, 1500, threshold=threshold)\n",
    "\n",
    "@tool\n",
    "def dcgm_gpu_inspect_tool(threshold: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Inspect raw GPU usage with model name, host, pod, and utilization.\n",
    "    Shows top 10 GPUs above threshold to prevent overwhelming output.\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    if not filtered:\n",
    "        return f\"✅ No GPUs over {threshold}% utilization.\"\n",
    "\n",
    "    # Limit to top 10 to prevent massive output\n",
    "    top = sorted(filtered, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [\n",
    "        [d[\"hostname\"][:20], d[\"gpu_id\"], d[\"model\"][:25], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"][:20]]\n",
    "        for d in top\n",
    "    ]\n",
    "    \n",
    "    from tabulate import tabulate\n",
    "    result = tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Util%\", \"Namespace\", \"Pod\"], tablefmt=\"grid\")\n",
    "    \n",
    "    # Add summary info\n",
    "    result += f\"\\n\\nShowing top 10 of {len(filtered)} GPUs above {threshold}% threshold.\"\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def calculate_dcgm_gpu_stats(threshold: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Analyze GPU utilization across nodes and return statistical breakdown.\n",
    "    Includes averages, idle/overloaded counts, and model/host distribution.\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    total = len(filtered)\n",
    "    if total == 0:\n",
    "        return f\"✅ No GPUs over the threshold of {threshold}% utilization.\"\n",
    "\n",
    "    avg_util = sum(d[\"utilization\"] for d in filtered) / total\n",
    "    maxed = [d for d in filtered if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in filtered if d[\"utilization\"] < 1.0]\n",
    "    moderate = [d for d in filtered if 1.0 <= d[\"utilization\"] < 70.0]\n",
    "    available = [d for d in filtered if d[\"utilization\"] < 100.0]\n",
    "    unique_models = set(d[\"model\"] for d in filtered)\n",
    "    unique_hosts = set(d[\"hostname\"] for d in filtered)\n",
    "\n",
    "    return f\"\"\"📊 GPU Utilization Stats (threshold: {threshold}%):\n",
    "\n",
    "🔍 Total GPUs: {total}\n",
    "📈 Average Utilization: {avg_util:.2f}%\n",
    "🔴 Fully Utilized (>=99%): {len(maxed)}\n",
    "🟢 Idle (<1%): {len(idle)}\n",
    "⚙️ Moderate (1-70%): {len(moderate)}\n",
    "💻 Unique Hosts: {len(unique_hosts)}\n",
    "🧠 Unique Models: {len(unique_models)}\n",
    "🧮 Available (<100%): {len(available)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c496a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRPModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.tools = []\n",
    "\n",
    "    def bind_tools(self, tools):\n",
    "        self.tools = tools\n",
    "        return self\n",
    "\n",
    "    def _convert_tool_to_openai_format(self, tool):\n",
    "        \"\"\"Convert LangChain tool to OpenAI tool format\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.args_schema.model_json_schema() if tool.args_schema else {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def invoke(self, messages):\n",
    "        # Convert messages to proper format\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            if hasattr(msg, 'content'):\n",
    "                if msg.__class__.__name__ == \"SystemMessage\":\n",
    "                    formatted_messages.append({\"role\": \"system\", \"content\": msg.content})\n",
    "                elif msg.__class__.__name__ == \"HumanMessage\":\n",
    "                    formatted_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "                elif msg.__class__.__name__ == \"AIMessage\":\n",
    "                    formatted_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "                elif msg.__class__.__name__ == \"ToolMessage\":\n",
    "                    # Truncate tool message content if too long\n",
    "                    content = str(msg.content)\n",
    "                    if len(content) > 2000:\n",
    "                        content = content[:2000] + \"\\n[Content truncated...]\"\n",
    "                    formatted_messages.append({\n",
    "                        \"role\": \"tool\", \n",
    "                        \"content\": content,\n",
    "                        \"tool_call_id\": getattr(msg, 'tool_call_id', 'unknown')\n",
    "                    })\n",
    "            else:\n",
    "                formatted_messages.append(msg)\n",
    "\n",
    "        # Convert tools to OpenAI format\n",
    "        openai_tools = None\n",
    "        if self.tools:\n",
    "            openai_tools = [self._convert_tool_to_openai_format(t) for t in self.tools]\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gemma3\",\n",
    "                temperature=0,\n",
    "                messages=formatted_messages,\n",
    "                tool_choice=\"auto\" if openai_tools else None,\n",
    "                tools=openai_tools,\n",
    "            )\n",
    "\n",
    "            choice = response.choices[0].message\n",
    "\n",
    "            tool_calls = []\n",
    "            if hasattr(choice, \"tool_calls\") and choice.tool_calls:\n",
    "                for t in choice.tool_calls:\n",
    "                    args = t.function.arguments\n",
    "                    if isinstance(args, str):\n",
    "                        try:\n",
    "                            args = json.loads(args)\n",
    "                        except json.JSONDecodeError:\n",
    "                            args = {}\n",
    "                    \n",
    "                    tool_calls.append({\n",
    "                        \"name\": t.function.name,\n",
    "                        \"args\": args,\n",
    "                        \"id\": t.id\n",
    "                    })\n",
    "\n",
    "            return AIMessage(\n",
    "                content=choice.content or \"\",\n",
    "                tool_calls=tool_calls\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return AIMessage(content=f\"Error calling model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "034ba055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe9a7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system: str = \"\"):\n",
    "        self.system = system\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "        self.max_iterations = 5  # Prevent infinite loops\n",
    "        self.current_iteration = 0\n",
    "\n",
    "        from langgraph.graph import StateGraph, END\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        self.raw_graph = graph\n",
    "        self.graph = graph.compile()\n",
    "\n",
    "    def exists_action(self, state: AgentState) -> bool:\n",
    "        \"\"\"Check if the last message has tool calls and we haven't exceeded max iterations\"\"\"\n",
    "        if self.current_iteration >= self.max_iterations:\n",
    "            print(f\"⚠️ Reached max iterations ({self.max_iterations}). Stopping.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            result = state[\"messages\"][-1]\n",
    "            return (hasattr(result, \"tool_calls\") and \n",
    "                    result.tool_calls is not None and \n",
    "                    len(result.tool_calls) > 0)\n",
    "        except (IndexError, KeyError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    def call_openai(self, state: AgentState) -> dict:\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState) -> dict:\n",
    "        self.current_iteration += 1\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        \n",
    "        for t in tool_calls:\n",
    "            tool_name = t[\"name\"]\n",
    "            tool_args = t[\"args\"]\n",
    "            print(f\"🔧 Calling tool: {tool_name} with args: {tool_args}\")\n",
    "            \n",
    "            if tool_name not in self.tools:\n",
    "                result = \"❌ Tool name not recognized. Available tools: \" + \", \".join(self.tools.keys())\n",
    "            else:\n",
    "                try:\n",
    "                    result = self.tools[tool_name].invoke(tool_args)\n",
    "                    # Ensure result is string and truncate if needed\n",
    "                    result = str(result)\n",
    "                    if len(result) > 3000:\n",
    "                        result = result[:3000] + \"\\n\\n[Output truncated to prevent loops]\"\n",
    "                except Exception as e:\n",
    "                    result = f\"❌ Tool error: {str(e)}\"\n",
    "            \n",
    "            results.append(ToolMessage(tool_call_id=t[\"id\"], name=tool_name, content=result))\n",
    "        \n",
    "        print(\"✅ Tool(s) executed. Returning to model.\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fab9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Updated system prompt to be more specific\n",
    "system_prompt = \"\"\"You are a Kubernetes monitoring assistant. \n",
    "\n",
    "Use these tools to answer questions:\n",
    "- 'describe_pods_tool': View pod/container info in a namespace\n",
    "- 'namespace_gpu_util_tool': View average GPU utilization per namespace  \n",
    "- 'dcgm_gpu_inspect_tool': View detailed GPU metrics (top 10 results)\n",
    "- 'calculate_dcgm_gpu_stats': Get statistical breakdown of GPU usage\n",
    "\n",
    "IMPORTANT: Only call each tool ONCE per question. Use the tool output to provide a direct answer. Do not repeat tool calls.\"\"\"\n",
    "\n",
    "# Create agent with updated tools\n",
    "model = NRPModel(client)\n",
    "tools = [describe_pods_tool, namespace_gpu_util_tool, dcgm_gpu_inspect_tool, calculate_dcgm_gpu_stats]\n",
    "abot = Agent(model=model, tools=tools, system=system_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc24ec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: List pods ===\n",
      "🔧 Calling tool: describe_pods_tool with args: {'namespace': 'gsoc'}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "\n",
      "\n",
      "Here are the pods currently running in the `gsoc` namespace:\n",
      "\n",
      "1. **agno-deployment-55c55964db-lzhkx**  \n",
      "   - Container: `jupyter`  \n",
      "   - Node: `hcc-nrp-shor-c6013.unl.edu`\n",
      "\n",
      "2. **my-postgres-cluster-0**  \n",
      "   - Container: `postgres`  \n",
      "   - Node: `k8s-gen4-02.ampath.net`\n",
      "\n",
      "3. **shellshock-cluster-0**  \n",
      "   - Container: `postgres`  \n",
      "   - Node: `dtn-gpu2.kreonet.net`\n",
      "\n",
      "Each pod is associated with its respective container and Kubernetes node host.\n",
      "\n",
      "=== Test 2: GPU usage by namespace ===\n",
      "🔧 Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "🔧 Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "\n",
      "\n",
      "Here's the GPU utilization breakdown across Kubernetes namespaces:\n",
      "\n",
      "**GPU Usage Summary by Namespace**  \n",
      "🟢 = Low utilization (<10% threshold)\n",
      "\n",
      "| Namespace                          | Avg GPU Utilization | Status |\n",
      "|------------------------------------|---------------------|--------|\n",
      "| sdsu-rci-jh                        | 7.69%               | 🟢 Low |\n",
      "| nrp-llm                            | 7.08%               | 🟢 Low |\n",
      "| gpu-mon                            | 0.15%               | 🟢 Low |\n",
      "| All others (7+ namespaces)         | 0.00%               | 🟢 Low |\n",
      "\n",
      "**Key Observations**  \n",
      "- Highest utilization seen in `sdsu-rci-jh` (7.69%) and `nrp-llm` (7.08%), but both remain in \"Low\" utilization category\n",
      "- 9+ namespaces show **0% GPU usage** (including `csusb-hpc`, `ucsb-cms-ml`, and research labs)\n",
      "- No namespaces currently exceed 10% average GPU utilization\n",
      "- System-wide GPU capacity appears significantly underutilized\n",
      "\n",
      "This suggests most workloads are either CPU-bound or not actively using GPU resources at this time.\n",
      "\n",
      "=== Test 3: GPU statistics ===\n",
      "🔧 Calling tool: calculate_dcgm_gpu_stats with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "🔧 Calling tool: calculate_dcgm_gpu_stats with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "\n",
      "\n",
      "The cluster's GPU statistics show **1,327 total GPUs** with an **average utilization of 26.29%**. Key metrics include:  \n",
      "- **194 GPUs** fully utilized (≥99% usage)  \n",
      "- **906 GPUs** idle (<1% usage)  \n",
      "- **86 GPUs** in moderate use (1-70% usage)  \n",
      "- **1,187 GPUs** available (<100% utilization)  \n",
      "- Spread across **226 unique hosts** and **25 GPU models**.  \n",
      "\n",
      "This indicates significant underutilization, with ~68% of GPUs idle or lightly used.\n"
     ]
    }
   ],
   "source": [
    "# Reset iteration counter before each use\n",
    "def ask_agent(question):\n",
    "    abot.current_iteration = 0  # Reset counter\n",
    "    messages = [HumanMessage(content=question)]\n",
    "    response = abot.graph.invoke({\"messages\": messages})\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "# Test cases\n",
    "print(\"=== Test 1: List pods ===\")\n",
    "print(ask_agent(\"List pods in gsoc namespace\"))\n",
    "\n",
    "print(\"\\n=== Test 2: GPU usage by namespace ===\")  \n",
    "print(ask_agent(\"Show me GPU usage across namespaces\"))\n",
    "\n",
    "print(\"\\n=== Test 3: GPU statistics ===\")\n",
    "print(ask_agent(\"Give me overall GPU statistics for the cluster\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "347cb9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complex A100 analysis query...\n",
      "🔧 Calling tool: calculate_dcgm_gpu_stats with args: {'threshold': 0.0}\n",
      "🔧 Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "\n",
      "\n",
      "### Analysis of NVIDIA A100 80GB PCIe GPUs\n",
      "\n",
      "#### 1. **Available vs Fully Utilized A100s**\n",
      "- **Fully Utilized (≥99%)**: **4 A100s** (all observed at **100% utilization** in the top GPU list).  \n",
      "- **Available (<100%)**: **0 observed** in the top utilization list (all visible A100s are fully utilized).  \n",
      "  *(Note: Total A100 count isn't explicitly provided in the data, but the top 10 GPUs include 4 A100s—all maxed out.)*\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. **Top Namespace Using A100s**\n",
      "- **`sdsc-llm`** is the **only namespace** observed using A100s (all 4 fully utilized GPUs).  \n",
      "  - Workload: `llama3-3-vllm-inferer` (LLM inference pod).  \n",
      "  - *No other namespaces are using A100s in the top utilization data.*\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. **Top A100s by Utilization (Host Details)**\n",
      "| Host                 | GPU | Model                 | Util%   | Namespace | Pod                  |\n",
      "|----------------------|-----|-----------------------|---------|-----------|----------------------|\n",
      "| node-2-10.sdsc.optip | 0   | NVIDIA A100 80GB PCIe | 100.00% | sdsc-llm  | llama3-3-vllm-inferer|\n",
      "| node-2-10.sdsc.optip | 1   | NVIDIA A100 80GB PCIe | 100.00% | sdsc-llm  | llama3-3-vllm-inferer|\n",
      "| node-2-10.sdsc.optip | 2   | NVIDIA A100 80GB PCIe | 100.00% | sdsc-llm  | llama3-3-vllm-inferer|\n",
      "| node-2-10.sdsc.optip | 3   | NVIDIA A100 80GB PCIe | 100.00% | sdsc-llm  | llama3-3-vllm-inferer|\n",
      "\n",
      "- **All 4 A100s** are on **`node-2-10.sdsc.optip`**, fully dedicated to LLM inference.\n",
      "\n",
      "---\n",
      "\n",
      "#### 4. **Overall A100 Statistics**\n",
      "- **Total Observed A100s**: **4** (all fully utilized).  \n",
      "- **Average Utilization**: **100%** (based on observed sample; no idle/moderate A100s in top utilization data).  \n",
      "- **Key Insight**: A100s are **exclusively used by `sdsc-llm`** for high-demand LLM workloads, with **zero idle capacity** in the observed dataset.  \n",
      "\n",
      "> 💡 **Recommendation**: Monitor `sdsc-llm` namespace for potential scaling needs—these A100s are at maximum capacity. No other namespaces are leveraging A100s in the current utilization snapshot.\n"
     ]
    }
   ],
   "source": [
    "# Complex A100 analysis queries you can try:\n",
    "\n",
    "# 1. Comprehensive A100 analysis\n",
    "query1 = \"\"\"\n",
    "Analyze all modelName=\"NVIDIA A100 80GB PCIe\" I want to know:\n",
    "- How many A100s are available vs fully utilized\n",
    "- Which namespaces are using A100s the most\n",
    "- Show me the top A100s by utilization with their host details\n",
    "- Give me overall statistics for A100s specifically\n",
    "\"\"\"\n",
    "\n",
    "# 2. A100 availability analysis\n",
    "query2 = \"\"\"\n",
    "I need to deploy a new workload that requires A100 GPUs. Can you:\n",
    "- Find all idle or low-utilization A100s (under 10% usage)\n",
    "- Show me which hosts have available A100s\n",
    "- Tell me which namespaces have the most A100 capacity available\n",
    "\"\"\"\n",
    "\n",
    "# 3. A100 performance comparison\n",
    "query3 = \"\"\"\n",
    "Compare A100 usage patterns across different namespaces:\n",
    "- Which namespace is using A100s most efficiently\n",
    "- Are there any A100s that are consistently underutilized\n",
    "- Show me the distribution of A100 utilization levels\n",
    "\"\"\"\n",
    "\n",
    "# 4. A100 resource optimization\n",
    "query4 = \"\"\"\n",
    "Help me optimize A100 resource allocation:\n",
    "- Find A100s with less than 50% utilization that could be reallocated\n",
    "- Identify hosts with mixed A100 utilization (some high, some low on same host)\n",
    "- Show me the overall A100 efficiency across the cluster\n",
    "\"\"\"\n",
    "\n",
    "# 5. Specific A100 investigation\n",
    "query5 = \"\"\"\n",
    "I'm investigating A100 performance issues. Please:\n",
    "- Show me all A100s with utilization above 95%\n",
    "- Identify any A100s that might be stuck or problematic (0% utilization)\n",
    "- Give me detailed host and pod information for the most utilized A100s\n",
    "\"\"\"\n",
    "\n",
    "# Test one of these complex queries\n",
    "print(\"Testing complex A100 analysis query...\")\n",
    "response = ask_agent(query1)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
