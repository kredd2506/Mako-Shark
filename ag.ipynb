{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Kubernetes Agentic Search System\n",
    "\n",
    "import requests\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_kubernetes_docs():\n",
    "    \"\"\"Load Kubernetes documentation by manually parsing the sitemap\"\"\"\n",
    "    \n",
    "    sitemap_url = \"https://kubernetes.io/en/sitemap.xml\"\n",
    "    filter_prefix = \"https://kubernetes.io/docs/\"\n",
    "    \n",
    "    print(\"Fetching sitemap...\")\n",
    "    try:\n",
    "        # Get the sitemap\n",
    "        response = requests.get(sitemap_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse XML using built-in ElementTree\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Extract URLs from sitemap\n",
    "        urls = []\n",
    "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        for url_elem in root.findall('.//ns:url', namespace):\n",
    "            loc_elem = url_elem.find('ns:loc', namespace)\n",
    "            if loc_elem is not None:\n",
    "                url_text = loc_elem.text\n",
    "                if url_text and url_text.startswith(filter_prefix):\n",
    "                    urls.append(url_text)\n",
    "        \n",
    "        print(f\"Found {len(urls)} Kubernetes documentation URLs\")\n",
    "        \n",
    "        # Load documents (limit for testing)\n",
    "        docs = []\n",
    "        max_docs = min(50, len(urls))\n",
    "        \n",
    "        for i, url in enumerate(urls[:max_docs]):\n",
    "            try:\n",
    "                print(f\"Loading {i+1}/{max_docs}: {url}\")\n",
    "                loader = WebBaseLoader(url)\n",
    "                doc_list = loader.load()\n",
    "                if doc_list:\n",
    "                    docs.extend(doc_list)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Failed to load {url}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nSuccessfully loaded {len(docs)} documents\")\n",
    "        return docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_kubernetes_docs_direct():\n",
    "    \"\"\"Load specific Kubernetes documentation pages directly\"\"\"\n",
    "    \n",
    "    key_urls = [\n",
    "        \"https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\",\n",
    "        \"https://kubernetes.io/docs/concepts/workloads/pods/\",\n",
    "        \"https://kubernetes.io/docs/concepts/services-networking/service/\",\n",
    "        \"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\",\n",
    "        \"https://kubernetes.io/docs/concepts/configuration/configmap/\",\n",
    "        \"https://kubernetes.io/docs/concepts/configuration/secret/\",\n",
    "        \"https://kubernetes.io/docs/concepts/services-networking/ingress/\",\n",
    "        \"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\",\n",
    "        \"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\",\n",
    "        \"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\",\n",
    "        \"https://kubernetes.io/docs/concepts/storage/volumes/\",\n",
    "        \"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\",\n",
    "        \"https://kubernetes.io/docs/concepts/cluster-administration/namespaces/\",\n",
    "        \"https://kubernetes.io/docs/reference/kubectl/cheatsheet/\",\n",
    "        \"https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/\",\n",
    "        \"https://kubernetes.io/docs/tutorials/kubernetes-basics/\",\n",
    "        \"https://kubernetes.io/docs/concepts/security/rbac/\",\n",
    "        \"https://kubernetes.io/docs/concepts/policy/resource-quotas/\",\n",
    "        \"https://kubernetes.io/docs/concepts/policy/limit-ranges/\",\n",
    "        \"https://kubernetes.io/docs/concepts/cluster-administration/networking/\"\n",
    "    ]\n",
    "    \n",
    "    docs = []\n",
    "    print(f\"Loading {len(key_urls)} key Kubernetes documentation pages...\")\n",
    "    \n",
    "    for i, url in enumerate(key_urls):\n",
    "        try:\n",
    "            print(f\"Loading {i+1}/{len(key_urls)}: {url.split('/')[-2]}\")\n",
    "            loader = WebBaseLoader(url)\n",
    "            doc_list = loader.load()\n",
    "            if doc_list:\n",
    "                docs.extend(doc_list)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to load {url}: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully loaded {len(docs)} key documentation pages\")\n",
    "    return docs\n",
    "\n",
    "def setup_embeddings_and_vectorstore():\n",
    "    \"\"\"Set up embeddings and vector store\"\"\"\n",
    "    \n",
    "    # Set OpenAI-compatible NRP API details\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"sk-XZib0Jr5mzGUgJxCDvt1Sg\"\n",
    "    os.environ[\"OPENAI_BASE_URL\"] = \"https://llm.nrp-nautilus.io/v1\"\n",
    "\n",
    "    # Test embedding endpoint\n",
    "    print(\"üìå Testing 'embed-mistral' model from NRP...\")\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=os.environ[\"OPENAI_BASE_URL\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        test = client.embeddings.create(\n",
    "            model=\"embed-mistral\",\n",
    "            input=[\"test embedding\"]\n",
    "        )\n",
    "        print(\"‚úÖ Embedding test successful. Response shape:\", len(test.data[0].embedding))\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Embedding model 'embed-mistral' failed:\", e)\n",
    "        raise e\n",
    "\n",
    "    # Initialize LangChain OpenAI-compatible embeddings\n",
    "    print(\"üîç Creating embeddings using NRP 'embed-mistral'...\")\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"embed-mistral\",\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=os.environ[\"OPENAI_BASE_URL\"]\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def create_vectorstore(docs, embeddings):\n",
    "    \"\"\"Create and save vector store\"\"\"\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    print(f\"Created {len(texts)} text chunks\")\n",
    "\n",
    "    # Convert documents to text strings\n",
    "    text_strings = [doc.page_content for doc in texts]\n",
    "    \n",
    "    # Build FAISS vector store\n",
    "    print(\"üì¶ Building vector store...\")\n",
    "    vectorstore = FAISS.from_texts(texts=text_strings, embedding=embeddings)\n",
    "\n",
    "    # Save vector store\n",
    "    vectorstore.save_local(\"kubernetes_vectorstore\")\n",
    "    print(\"‚úÖ Vector store saved locally.\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "def setup_qa_chain(vectorstore):\n",
    "    \"\"\"Set up the QA chain with retriever\"\"\"\n",
    "    \n",
    "    print(\"\\nSetting up retrieval system...\")\n",
    "\n",
    "    # Create retriever with multiple search strategies\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",  # Maximum Marginal Relevance for diverse results\n",
    "        search_kwargs={\n",
    "            \"k\": 8,  # Retrieve top 8 most relevant chunks\n",
    "            \"fetch_k\": 20,  # Fetch 20 candidates before MMR selection\n",
    "            \"lambda_mult\": 0.7  # Diversity vs relevance balance\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the language model (using NRP endpoint)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4-turbo-preview\",  # Change this to available model on NRP\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000,\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=os.environ[\"OPENAI_BASE_URL\"]\n",
    "    )\n",
    "\n",
    "    # Create Agentic Search Template\n",
    "    agentic_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a Kubernetes expert assistant with access to official documentation. \n",
    "Your goal is to provide accurate, helpful answers about Kubernetes concepts, troubleshooting, and best practices.\n",
    "\n",
    "Context from Kubernetes documentation:\n",
    "{context}\n",
    "\n",
    "Human question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the question to understand what the user needs\n",
    "2. Use the provided context to give accurate information\n",
    "3. If the context doesn't fully answer the question, clearly state what information is missing\n",
    "4. Provide practical examples when relevant\n",
    "5. Suggest follow-up questions or related topics that might be helpful\n",
    "6. If you detect the user might need step-by-step guidance, offer to break down complex tasks\n",
    "\n",
    "Answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Create the QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": agentic_prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"Agentic search system ready!\")\n",
    "    return qa_chain\n",
    "\n",
    "def agentic_search(qa_chain, question, show_sources=True):\n",
    "    \"\"\"Perform agentic search on Kubernetes documentation\"\"\"\n",
    "    \n",
    "    print(f\"üîç Searching for: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get answer and sources\n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(\"üìã Answer:\")\n",
    "    print(result[\"result\"])\n",
    "    \n",
    "    if show_sources and result.get(\"source_documents\"):\n",
    "        print(\"\\nüìö Sources:\")\n",
    "        for i, doc in enumerate(result[\"source_documents\"][:3], 1):\n",
    "            source_url = doc.metadata.get(\"source\", \"Unknown\")\n",
    "            print(f\"{i}. {source_url}\")\n",
    "            print(f\"   Preview: {doc.page_content[:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    return result\n",
    "\n",
    "class KubernetesAgent:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def search_with_context(self, question):\n",
    "        \"\"\"Search with conversation context\"\"\"\n",
    "        \n",
    "        # Add conversation context to question if available\n",
    "        if self.conversation_history:\n",
    "            context_question = f\"\"\"\n",
    "            Previous conversation context:\n",
    "            {' '.join(self.conversation_history[-3:])}\n",
    "            \n",
    "            Current question: {question}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            context_question = question\n",
    "        \n",
    "        result = self.qa_chain({\"query\": context_question})\n",
    "        \n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append(f\"Q: {question}\")\n",
    "        self.conversation_history.append(f\"A: {result['result'][:200]}...\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def suggest_follow_ups(self, question, answer):\n",
    "        \"\"\"Generate follow-up questions based on the current Q&A\"\"\"\n",
    "        \n",
    "        follow_up_prompt = f\"\"\"\n",
    "        Based on this Kubernetes Q&A:\n",
    "        Q: {question}\n",
    "        A: {answer[:300]}...\n",
    "        \n",
    "        Suggest 3 relevant follow-up questions a user might ask.\n",
    "        Return only the questions, one per line.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            follow_up_result = self.qa_chain.llm.invoke(follow_up_prompt)\n",
    "            return follow_up_result.content.strip().split('\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating follow-ups: {e}\")\n",
    "            return [\"Unable to generate follow-up questions at this time.\"]\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive search session\"\"\"\n",
    "        print(\"ü§ñ Kubernetes Agent activated! Ask me anything about Kubernetes.\")\n",
    "        print(\"Type 'quit' to exit, 'clear' to clear history\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\n‚ùì Your question: \").strip()\n",
    "            \n",
    "            if question.lower() == 'quit':\n",
    "                break\n",
    "            elif question.lower() == 'clear':\n",
    "                self.conversation_history = []\n",
    "                print(\"üìù Conversation history cleared!\")\n",
    "                continue\n",
    "            elif not question:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Get answer\n",
    "                result = self.search_with_context(question)\n",
    "                print(f\"\\nü§ñ Answer: {result['result']}\")\n",
    "                \n",
    "                # Show sources\n",
    "                if result.get(\"source_documents\"):\n",
    "                    print(f\"\\nüìö Key sources: {len(result['source_documents'])} documents referenced\")\n",
    "                \n",
    "                # Suggest follow-ups\n",
    "                follow_ups = self.suggest_follow_ups(question, result['result'])\n",
    "                print(\"\\nüí° You might also want to ask:\")\n",
    "                for i, follow_up in enumerate(follow_ups[:3], 1):\n",
    "                    if follow_up.strip():\n",
    "                        print(f\"   {i}. {follow_up.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing your question: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to set up and run the Kubernetes agent\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Initializing Kubernetes Agentic Search System...\")\n",
    "    \n",
    "    # Step 1: Load documents\n",
    "    print(\"üìÑ Loading Kubernetes documentation...\")\n",
    "    docs = load_kubernetes_docs()\n",
    "    \n",
    "    if len(docs) == 0:\n",
    "        print(\"\\nSitemap approach failed, using direct URL approach...\")\n",
    "        docs = load_kubernetes_docs_direct()\n",
    "    \n",
    "    if len(docs) == 0:\n",
    "        print(\"‚ùå Failed to load any documents. Please check your internet connection.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Set up embeddings\n",
    "    print(\"üîß Setting up embeddings...\")\n",
    "    embeddings = setup_embeddings_and_vectorstore()\n",
    "    \n",
    "    # Step 3: Create vector store\n",
    "    print(\"üì¶ Creating vector store...\")\n",
    "    vectorstore = create_vectorstore(docs, embeddings)\n",
    "    \n",
    "    # Step 4: Set up QA chain\n",
    "    print(\"‚öôÔ∏è Setting up QA chain...\")\n",
    "    qa_chain = setup_qa_chain(vectorstore)\n",
    "    \n",
    "    # Step 5: Initialize agent\n",
    "    print(\"ü§ñ Initializing Kubernetes Agent...\")\n",
    "    k8s_agent = KubernetesAgent(qa_chain)\n",
    "    \n",
    "    print(\"\\n‚úÖ Kubernetes Agentic Search System Ready!\")\n",
    "    print(\"\\nAvailable functions:\")\n",
    "    print(\"- agentic_search(qa_chain, 'your question here')\")\n",
    "    print(\"- k8s_agent.interactive_search()  # For interactive session\")\n",
    "    print(\"- k8s_agent.search_with_context('your question')  # With conversation memory\")\n",
    "    \n",
    "    # Test with a sample question\n",
    "    test_question = \"What is a Pod in Kubernetes and how does it work?\"\n",
    "    print(f\"\\nüß™ Testing with sample question: {test_question}\")\n",
    "    result = agentic_search(qa_chain, test_question)\n",
    "    \n",
    "    return qa_chain, k8s_agent\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    qa_chain, k8s_agent = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
