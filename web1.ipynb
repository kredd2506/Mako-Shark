{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Combined Agentic Search & Prometheus Monitoring\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.exceptions import ApiException\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tabulate import tabulate\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Setup - Replace with your actual API key\n",
    "os.environ[\"NRP_API_KEY\"] = \"NRP-API-key-here\"  # Replace with your actual key\n",
    "config.load_incluster_config()\n",
    "v1 = client.CoreV1Api()\n",
    "apps_v1 = client.AppsV1Api()\n",
    "batch_v1 = client.BatchV1Api()\n",
    "networking_v1 = client.NetworkingV1Api()\n",
    "\n",
    "# OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"NRP_API_KEY\"),\n",
    "    base_url=\"https://llm.nrp-nautilus.io/\"\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# Documentation Knowledge Base\n",
    "# %%\n",
    "class DocumentationKnowledgeBase:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.metadata = []\n",
    "        self.api_key = os.environ.get(\"NRP_API_KEY\", \"NRP-API-key-here\")\n",
    "        self.base_url = \"https://llm.nrp-nautilus.io/\"\n",
    "        self.embedding_endpoint = f\"{self.base_url}/v1/embeddings\"\n",
    "        self.rerank_endpoint = f\"{self.base_url}/v1/rerank\"\n",
    "        \n",
    "        # Create a robust session with retries\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\", \"POST\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": \"NRP-Documentation-Crawler/1.0\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        })\n",
    "        \n",
    "    def crawl_documentation(self, base_url, max_depth=1, delay=2, timeout=30):\n",
    "        \"\"\"Crawl the NRP.ai documentation with specified depth\"\"\"\n",
    "        visited_urls = set()\n",
    "        pages_to_crawl = [(base_url, 0)]\n",
    "        failed_urls = []\n",
    "        \n",
    "        while pages_to_crawl:\n",
    "            url, depth = pages_to_crawl.pop(0)\n",
    "            \n",
    "            if depth > max_depth or url in visited_urls:\n",
    "                continue\n",
    "                \n",
    "            visited_urls.add(url)\n",
    "            time.sleep(delay)  # Delay between requests\n",
    "            \n",
    "            try:\n",
    "                print(f\"Crawling: {url} (depth: {depth})\")\n",
    "                response = self.session.get(url, timeout=timeout)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Failed to fetch {url}: Status {response.status_code}\")\n",
    "                    failed_urls.append(url)\n",
    "                    continue\n",
    "                    \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Extract page content\n",
    "                title = soup.find('title').get_text() if soup.find('title') else \"No Title\"\n",
    "                \n",
    "                # Try to find the main content area\n",
    "                content_div = soup.find('div', class_='documentation') or \\\n",
    "                              soup.find('main') or \\\n",
    "                              soup.find('article') or \\\n",
    "                              soup.find('div', class_='content') or \\\n",
    "                              soup\n",
    "                \n",
    "                content = content_div.get_text(strip=True)\n",
    "                \n",
    "                # Skip pages with very little content\n",
    "                if len(content) < 100:\n",
    "                    print(f\"Skipping {url} - insufficient content\")\n",
    "                    continue\n",
    "                \n",
    "                # Store document with metadata\n",
    "                self.documents.append({\n",
    "                    'text': content,\n",
    "                    'url': url,\n",
    "                    'title': title\n",
    "                })\n",
    "                \n",
    "                # Find all links\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    href = link['href']\n",
    "                    full_url = urljoin(url, href)\n",
    "                    \n",
    "                    # Only follow links within the documentation\n",
    "                    if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "                        pages_to_crawl.append((full_url, depth + 1))\n",
    "                        \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error crawling {url}: {e}\")\n",
    "                failed_urls.append(url)\n",
    "                continue\n",
    "                \n",
    "        print(f\"Crawled {len(self.documents)} pages\")\n",
    "        if failed_urls:\n",
    "            print(f\"Failed to crawl {len(failed_urls)} pages:\")\n",
    "            for url in failed_urls:\n",
    "                print(f\"  - {url}\")\n",
    "                \n",
    "    def get_embeddings(self, texts, batch_size=10):\n",
    "        \"\"\"Get embeddings from the NRP API with batching\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process texts in batches to avoid overwhelming the API\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            data = {\n",
    "                \"model\": \"embed-mistral\",\n",
    "                \"input\": batch\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = self.session.post(self.embedding_endpoint, json=data, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    all_embeddings.extend([item['embedding'] for item in result['data']])\n",
    "                    print(f\"Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "                else:\n",
    "                    print(f\"Error getting embeddings: {response.status_code} - {response.text}\")\n",
    "                    # Add zero embeddings as fallback\n",
    "                    all_embeddings.extend([[0.0] * 768] * len(batch))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception when getting embeddings: {e}\")\n",
    "                # Add zero embeddings as fallback\n",
    "                all_embeddings.extend([[0.0] * 768] * len(batch))\n",
    "                \n",
    "            # Add delay between batches\n",
    "            time.sleep(1)\n",
    "            \n",
    "        return all_embeddings\n",
    "    \n",
    "    def rerank_results(self, query, documents, top_k=5):\n",
    "        \"\"\"Rerank search results using the NRP API\"\"\"\n",
    "        # Prepare documents for reranking\n",
    "        docs_for_rerank = [{\"text\": doc['text']} for doc in documents]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"gemma3\",\n",
    "            \"query\": query,\n",
    "            \"documents\": docs_for_rerank,\n",
    "            \"top_n\": top_k\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(self.rerank_endpoint, json=data, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                # Get indices of top results\n",
    "                top_indices = [item['index'] for item in result['results']]\n",
    "                return [documents[i] for i in top_indices]\n",
    "            else:\n",
    "                print(f\"Error reranking results: {response.status_code} - {response.text}\")\n",
    "                return documents[:top_k]  # Fallback to original order\n",
    "        except Exception as e:\n",
    "            print(f\"Exception when reranking: {e}\")\n",
    "            return documents[:top_k]  # Fallback to original order\n",
    "    \n",
    "    def search(self, query, top_k=5, use_reranking=True):\n",
    "        \"\"\"Search the knowledge base\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            print(\"Knowledge base not loaded. Please load it first.\")\n",
    "            return []\n",
    "            \n",
    "        # Get query embedding\n",
    "        query_embedding = self.get_embeddings([query])\n",
    "        if query_embedding is None:\n",
    "            print(\"Failed to generate query embedding\")\n",
    "            return []\n",
    "            \n",
    "        query_embedding = np.array(query_embedding[0]).reshape(1, -1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings).flatten()\n",
    "        \n",
    "        # Get top results\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': self.documents[idx]['text'],\n",
    "                'url': self.metadata[idx]['url'],\n",
    "                'title': self.metadata[idx]['title'],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "        \n",
    "        # Apply reranking if requested\n",
    "        if use_reranking and len(results) > 0:\n",
    "            results = self.rerank_results(query, results, top_k)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def build_knowledge_base(self):\n",
    "        \"\"\"Build the knowledge base with embeddings\"\"\"\n",
    "        if not self.documents:\n",
    "            print(\"No documents to process. Please crawl the documentation first.\")\n",
    "            return\n",
    "            \n",
    "        # Get embeddings for all documents\n",
    "        print(\"Generating embeddings...\")\n",
    "        texts = [doc['text'] for doc in self.documents]\n",
    "        embeddings = self.get_embeddings(texts)\n",
    "        \n",
    "        if embeddings is None:\n",
    "            print(\"Failed to generate embeddings\")\n",
    "            return\n",
    "            \n",
    "        self.embeddings = np.array(embeddings)\n",
    "        self.metadata = [{\n",
    "            'url': doc['url'],\n",
    "            'title': doc['title']\n",
    "        } for doc in self.documents]\n",
    "        \n",
    "        print(f\"Knowledge base built with {len(self.documents)} documents\")\n",
    "    \n",
    "    def save_knowledge_base(self, filepath):\n",
    "        \"\"\"Save the knowledge base to disk\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            print(\"Knowledge base not built. Nothing to save.\")\n",
    "            return\n",
    "            \n",
    "        data = {\n",
    "            'documents': self.documents,\n",
    "            'embeddings': self.embeddings.tolist(),\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "            \n",
    "        print(f\"Knowledge base saved to {filepath}\")\n",
    "    \n",
    "    def load_knowledge_base(self, filepath):\n",
    "        \"\"\"Load the knowledge base from disk\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        self.documents = data['documents']\n",
    "        self.embeddings = np.array(data['embeddings'])\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        print(f\"Knowledge base loaded from {filepath} with {len(self.documents)} documents\")\n",
    "\n",
    "# Initialize Documentation Knowledge Base\n",
    "doc_kb = DocumentationKnowledgeBase()\n",
    "kb_file = \"nrp_expert_docs_kb.json\"\n",
    "if os.path.exists(kb_file):\n",
    "    doc_kb.load_knowledge_base(kb_file)\n",
    "else:\n",
    "    print(f\"Knowledge base file {kb_file} not found. Building it now with crawl depth 1...\")\n",
    "    doc_kb.crawl_documentation(\"https://nrp.ai/documentation/\", max_depth=1)\n",
    "    doc_kb.build_knowledge_base()\n",
    "    doc_kb.save_knowledge_base(kb_file)\n",
    "\n",
    "# %% [markdown]\n",
    "# Prometheus Monitoring Functions\n",
    "# %%\n",
    "def describe_pods(namespace=\"gsoc\"):\n",
    "    \"\"\"Describe pods and print only fields useful for Prometheus metric queries.\"\"\"\n",
    "    try:\n",
    "        pods = v1.list_namespaced_pod(namespace=namespace) if namespace else v1.list_pod_for_all_namespaces()\n",
    "        rows = []\n",
    "        for pod in pods.items:\n",
    "            pod_name = pod.metadata.name\n",
    "            ns = pod.metadata.namespace\n",
    "            pod_ip = pod.status.pod_ip\n",
    "            node = pod.spec.node_name\n",
    "            container_names = [c.name for c in pod.spec.containers]\n",
    "            container = \", \".join(container_names)\n",
    "            rows.append([pod_name, ns, pod_ip, node, container])\n",
    "        headers = [\"Pod\", \"Namespace\", \"Pod IP\", \"Node\", \"Container\"]\n",
    "        return tabulate(rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "    except ApiException as e:\n",
    "        return f\"‚ùå Error fetching pods: {e}\"\n",
    "\n",
    "def namespace_gpu_utilization(prom_url=\"https://prometheus.nrp-nautilus.io\", threshold=0):\n",
    "    \"\"\"Display average GPU utilization per namespace using PromQL.\"\"\"\n",
    "    query = 'avg by (namespace) (DCGM_FI_DEV_GPU_UTIL)'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            return \"‚ùå Prometheus query failed.\"\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            return \"‚úÖ Query successful, but no GPU usage data returned.\"\n",
    "        rows = []\n",
    "        for r in results:\n",
    "            ns = r[\"metric\"].get(\"namespace\", \"unknown\")\n",
    "            util = float(r[\"value\"][1])\n",
    "            if util >= threshold:\n",
    "                status = (\n",
    "                    \"üü¢ Low\" if util < 40 else\n",
    "                    \"üü° Moderate\" if util < 70 else\n",
    "                    \"üî¥ High\"\n",
    "                )\n",
    "                rows.append([ns, f\"{util:.2f}%\", status])\n",
    "        headers = [\"Namespace\", \"Avg GPU Utilization\", \"Status\"]\n",
    "        return tabulate(rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error querying Prometheus: {e}\"\n",
    "\n",
    "def fetch_dcgm_gpu_util_data(prom_url=\"https://prometheus.nrp-nautilus.io\"):\n",
    "    \"\"\"Fetch rich GPU utilization data from Prometheus using DCGM_FI_DEV_GPU_UTIL.\"\"\"\n",
    "    query = 'DCGM_FI_DEV_GPU_UTIL'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            return []\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            return []\n",
    "        enriched = []\n",
    "        for r in results:\n",
    "            m = r[\"metric\"]\n",
    "            val = float(r[\"value\"][1])\n",
    "            enriched.append({\n",
    "                \"hostname\": m.get(\"Hostname\", \"unknown\"),\n",
    "                \"ip_port\": m.get(\"instance\", \"unknown\"),\n",
    "                \"gpu_id\": m.get(\"gpu\", \"N/A\"),\n",
    "                \"device\": m.get(\"device\", \"N/A\"),\n",
    "                \"uuid\": m.get(\"UUID\", \"N/A\"),\n",
    "                \"model\": m.get(\"modelName\", \"unknown\"),\n",
    "                \"namespace\": m.get(\"namespace\", \"N/A\"),\n",
    "                \"pod\": m.get(\"pod\", \"N/A\"),\n",
    "                \"utilization\": val\n",
    "            })\n",
    "        return enriched\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def analyze_dcgm_gpu_data(data):\n",
    "    \"\"\"Analyze DCGM GPU data with statistics and top utilization.\"\"\"\n",
    "    if not data:\n",
    "        return \"No data to analyze.\"\n",
    "    total = len(data)\n",
    "    avg_util = sum(d[\"utilization\"] for d in data) / total\n",
    "    maxed = [d for d in data if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in data if d[\"utilization\"] < 1.0]\n",
    "    available = [d for d in data if d[\"utilization\"] < 100.0]\n",
    "    unique_hosts = set(d[\"hostname\"] for d in data)\n",
    "    unique_models = set(d[\"model\"] for d in data)\n",
    "    \n",
    "    result = f\"\"\"üîç Total GPUs: {total}\n",
    "üìä Average Utilization: {avg_util:.2f}%\n",
    "üî¥ Fully Utilized GPUs (>=99%): {len(maxed)}\n",
    "üü¢ Idle GPUs (<1%): {len(idle)}\n",
    "üíª Unique Host Machines: {len(unique_hosts)}\n",
    "üß† Unique GPU Models: {len(unique_models)}\n",
    "üßÆ GPUs Available (<100%): {len(available)}\\n\"\"\"\n",
    "    \n",
    "    result += \"üìà Top 10 GPUs by Utilization:\\n\"\n",
    "    top = sorted(data, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [[d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]] for d in top]\n",
    "    result += tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"github\")\n",
    "    return result\n",
    "\n",
    "# %% [markdown]\n",
    "# Tool Definitions\n",
    "# %%\n",
    "# Helper to capture and truncate printed output\n",
    "def capture_stdout_truncated(func, max_length=2000, *args, **kwargs):\n",
    "    \"\"\"Capture stdout and truncate if too long to prevent LLM loops\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    output = mystdout.getvalue()\n",
    "    if len(output) > max_length:\n",
    "        output = output[:max_length] + f\"\\n\\n... [Output truncated - showing first {max_length} characters]\"\n",
    "    return output\n",
    "\n",
    "# Documentation search tool\n",
    "@tool\n",
    "def search_documentation(query: str) -> str:\n",
    "    \"\"\"Search the NRP.ai documentation for relevant information.\"\"\"\n",
    "    if doc_kb.embeddings is None:\n",
    "        return \"‚ùå Knowledge base not loaded. Cannot search documentation.\"\n",
    "    \n",
    "    results = doc_kb.search(query, top_k=3)\n",
    "    if not results:\n",
    "        return \"‚ùå No relevant documentation found.\"\n",
    "    \n",
    "    output = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"Result {i}:\")\n",
    "        output.append(f\"Title: {result['title']}\")\n",
    "        output.append(f\"URL: {result['url']}\")\n",
    "        output.append(f\"Content: {result['text'][:200]}...\")\n",
    "        output.append(\"\")  # Empty line\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# GPU monitoring tools\n",
    "@tool\n",
    "def describe_pods_tool(namespace: str = \"gsoc\") -> str:\n",
    "    \"\"\"Describe pods in a given Kubernetes namespace. Defaults to 'gsoc'.\"\"\"\n",
    "    return capture_stdout_truncated(describe_pods, 1500, namespace=namespace)\n",
    "\n",
    "@tool\n",
    "def namespace_gpu_util_tool(threshold: float = 0.0) -> str:\n",
    "    \"\"\"Get average GPU utilization per namespace with optional threshold filter.\"\"\"\n",
    "    return capture_stdout_truncated(namespace_gpu_utilization, 1500, threshold=threshold)\n",
    "\n",
    "@tool\n",
    "def dcgm_gpu_inspect_tool(threshold: float = 0.0) -> str:\n",
    "    \"\"\"Inspect raw GPU usage with model name, host, pod, and utilization. Shows top 10 GPUs above threshold.\"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"‚ö†Ô∏è No GPU data available.\"\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    if not filtered:\n",
    "        return f\"‚úÖ No GPUs over {threshold}% utilization.\"\n",
    "    top = sorted(filtered, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [\n",
    "        [d[\"hostname\"][:20], d[\"gpu_id\"], d[\"model\"][:25], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"][:20]]\n",
    "        for d in top\n",
    "    ]\n",
    "    result = tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Util%\", \"Namespace\", \"Pod\"], tablefmt=\"grid\")\n",
    "    result += f\"\\n\\nShowing top 10 of {len(filtered)} GPUs above {threshold}% threshold.\"\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def calculate_dcgm_gpu_stats(threshold: float = 0.0) -> str:\n",
    "    \"\"\"Analyze GPU utilization across nodes and return statistical breakdown.\"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"‚ö†Ô∏è No GPU data available.\"\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    total = len(filtered)\n",
    "    if total == 0:\n",
    "        return f\"‚úÖ No GPUs over the threshold of {threshold}% utilization.\"\n",
    "    avg_util = sum(d[\"utilization\"] for d in filtered) / total\n",
    "    maxed = [d for d in filtered if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in filtered if d[\"utilization\"] < 1.0]\n",
    "    moderate = [d for d in filtered if 1.0 <= d[\"utilization\"] < 70.0]\n",
    "    available = [d for d in filtered if d[\"utilization\"] < 100.0]\n",
    "    unique_models = set(d[\"model\"] for d in filtered)\n",
    "    unique_hosts = set(d[\"hostname\"] for d in filtered)\n",
    "    return f\"\"\"üìä GPU Utilization Stats (threshold: {threshold}%):\n",
    "üîç Total GPUs: {total}\n",
    "üìà Average Utilization: {avg_util:.2f}%\n",
    "üî¥ Fully Utilized (>=99%): {len(maxed)}\n",
    "üü¢ Idle (<1%): {len(idle)}\n",
    "‚öôÔ∏è Moderate (1-70%): {len(moderate)}\n",
    "üíª Unique Hosts: {len(unique_hosts)}\n",
    "üß† Unique Models: {len(unique_models)}\n",
    "üßÆ Available (<100%): {len(available)}\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# LangGraph Agent Implementation\n",
    "# %%\n",
    "class NRPModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.tools = []\n",
    "    def bind_tools(self, tools):\n",
    "        self.tools = tools\n",
    "        return self\n",
    "    def _convert_tool_to_openai_format(self, tool):\n",
    "        \"\"\"Convert LangChain tool to OpenAI tool format\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.args_schema.model_json_schema() if tool.args_schema else {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    def invoke(self, messages):\n",
    "        # Convert messages to proper format\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            if hasattr(msg, 'content'):\n",
    "                if msg.__class__.__name__ == \"SystemMessage\":\n",
    "                    formatted_messages.append({\"role\": \"system\", \"content\": msg.content})\n",
    "                elif msg.__class__.__name__ == \"HumanMessage\":\n",
    "                    formatted_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "                elif msg.__class__.__name__ == \"AIMessage\":\n",
    "                    formatted_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "                elif msg.__class__.__name__ == \"ToolMessage\":\n",
    "                    # Truncate tool message content if too long\n",
    "                    content = str(msg.content)\n",
    "                    if len(content) > 2000:\n",
    "                        content = content[:2000] + \"\\n[Content truncated...]\"\n",
    "                    formatted_messages.append({\n",
    "                        \"role\": \"tool\", \n",
    "                        \"content\": content,\n",
    "                        \"tool_call_id\": getattr(msg, 'tool_call_id', 'unknown')\n",
    "                    })\n",
    "            else:\n",
    "                formatted_messages.append(msg)\n",
    "        # Convert tools to OpenAI format\n",
    "        openai_tools = None\n",
    "        if self.tools:\n",
    "            openai_tools = [self._convert_tool_to_openai_format(t) for t in self.tools]\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gemma3\",\n",
    "                temperature=0,\n",
    "                messages=formatted_messages,\n",
    "                tool_choice=\"auto\" if openai_tools else None,\n",
    "                tools=openai_tools,\n",
    "            )\n",
    "            choice = response.choices[0].message\n",
    "            tool_calls = []\n",
    "            if hasattr(choice, \"tool_calls\") and choice.tool_calls:\n",
    "                for t in choice.tool_calls:\n",
    "                    args = t.function.arguments\n",
    "                    if isinstance(args, str):\n",
    "                        try:\n",
    "                            args = json.loads(args)\n",
    "                        except json.JSONDecodeError:\n",
    "                            args = {}\n",
    "                    tool_calls.append({\n",
    "                        \"name\": t.function.name,\n",
    "                        \"args\": args,\n",
    "                        \"id\": t.id\n",
    "                    })\n",
    "            return AIMessage(\n",
    "                content=choice.content or \"\",\n",
    "                tool_calls=tool_calls\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return AIMessage(content=f\"Error calling model: {str(e)}\")\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system: str = \"\"):\n",
    "        self.system = system\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "        self.max_iterations = 5  # Prevent infinite loops\n",
    "        self.current_iteration = 0\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.raw_graph = graph\n",
    "        self.graph = graph.compile()\n",
    "    \n",
    "    def exists_action(self, state: AgentState) -> bool:\n",
    "        \"\"\"Check if the last message has tool calls and we haven't exceeded max iterations\"\"\"\n",
    "        if self.current_iteration >= self.max_iterations:\n",
    "            print(f\"‚ö†Ô∏è Reached max iterations ({self.max_iterations}). Stopping.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            result = state[\"messages\"][-1]\n",
    "            return (hasattr(result, \"tool_calls\") and \n",
    "                    result.tool_calls is not None and \n",
    "                    len(result.tool_calls) > 0)\n",
    "        except (IndexError, KeyError, AttributeError):\n",
    "            return False\n",
    "    \n",
    "    def call_openai(self, state: AgentState) -> dict:\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState) -> dict:\n",
    "        self.current_iteration += 1\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        \n",
    "        for t in tool_calls:\n",
    "            tool_name = t[\"name\"]\n",
    "            tool_args = t[\"args\"]\n",
    "            print(f\"üîß Calling tool: {tool_name} with args: {tool_args}\")\n",
    "            \n",
    "            if tool_name not in self.tools:\n",
    "                result = \"‚ùå Tool name not recognized. Available tools: \" + \", \".join(self.tools.keys())\n",
    "            else:\n",
    "                try:\n",
    "                    result = self.tools[tool_name].invoke(tool_args)\n",
    "                    # Ensure result is string and truncate if needed\n",
    "                    result = str(result)\n",
    "                    if len(result) > 3000:\n",
    "                        result = result[:3000] + \"\\n\\n[Output truncated to prevent loops]\"\n",
    "                except Exception as e:\n",
    "                    result = f\"‚ùå Tool error: {str(e)}\"\n",
    "            \n",
    "            results.append(ToolMessage(tool_call_id=t[\"id\"], name=tool_name, content=result))\n",
    "        \n",
    "        print(\"‚úÖ Tool(s) executed. Returning to model.\")\n",
    "        return {\"messages\": results}\n",
    "\n",
    "# %% [markdown]\n",
    "# Agent Initialization\n",
    "# %%\n",
    "# System prompt combining documentation search and monitoring\n",
    "system_prompt = \"\"\"You are a Kubernetes monitoring assistant with access to NRP.ai documentation. \n",
    "Use these tools to answer questions:\n",
    "- 'search_documentation': Search NRP.ai documentation for conceptual information\n",
    "- 'describe_pods_tool': View pod/container info in a namespace\n",
    "- 'namespace_gpu_util_tool': View average GPU utilization per namespace  \n",
    "- 'dcgm_gpu_inspect_tool': View detailed GPU metrics (top 10 results)\n",
    "- 'calculate_dcgm_gpu_stats': Get statistical breakdown of GPU usage\n",
    "\n",
    "Guidelines:\n",
    "1. For conceptual questions about Kubernetes, GPU usage, or cloud computing, use 'search_documentation'\n",
    "2. For current cluster status or resource utilization questions, use the monitoring tools\n",
    "3. Only call each tool ONCE per question\n",
    "4. Use the tool output to provide a direct answer\n",
    "5. Do not repeat tool calls\n",
    "6. For complex queries, break them down into multiple tool calls if needed\"\"\"\n",
    "\n",
    "# Create agent with combined tools\n",
    "model = NRPModel(client)\n",
    "tools = [\n",
    "    search_documentation,\n",
    "    describe_pods_tool,\n",
    "    namespace_gpu_util_tool,\n",
    "    dcgm_gpu_inspect_tool,\n",
    "    calculate_dcgm_gpu_stats\n",
    "]\n",
    "abot = Agent(model=model, tools=tools, system=system_prompt)\n",
    "\n",
    "# %% [markdown]\n",
    "# Query Function\n",
    "# %%\n",
    "def ask_agent(question):\n",
    "    \"\"\"Ask the agent a question and get a response\"\"\"\n",
    "    abot.current_iteration = 0  # Reset counter\n",
    "    messages = [HumanMessage(content=question)]\n",
    "    response = abot.graph.invoke({\"messages\": messages})\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "# %% [markdown]\n",
    "# Test Cases\n",
    "# %%\n",
    "print(\"=== Test 1: Documentation Search ===\")\n",
    "print(ask_agent(\"How does GPU scheduling work in Kubernetes?\"))\n",
    "\n",
    "print(\"\\n=== Test 2: Current GPU Utilization ===\")\n",
    "print(ask_agent(\"What's the current GPU utilization across all namespaces?\"))\n",
    "\n",
    "print(\"\\n=== Test 3: Detailed GPU Analysis ===\")\n",
    "print(ask_agent(\"Show me detailed GPU statistics and identify any idle GPUs\"))\n",
    "\n",
    "print(\"\\n=== Test 4: Combined Query ===\")\n",
    "print(ask_agent(\"I'm having issues with GPU scheduling. Can you check current utilization and explain best practices?\"))\n",
    "\n",
    "print(\"\\n=== Test 5: A100 Analysis ===\")\n",
    "print(ask_agent(\"Analyze all A100 GPUs in the cluster. Show utilization, availability, and which namespaces are using them\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
