{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tabulate import tabulate\n",
    "# %%\n",
    "os.environ[\"NRP_API_KEY\"] = \"NRP-API-key-here\"\n",
    "# %%\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client, config\n",
    "from kubernetes.client.exceptions import ApiException\n",
    "config.load_incluster_config()\n",
    "v1 = client.CoreV1Api()\n",
    "apps_v1 = client.AppsV1Api()\n",
    "batch_v1 = client.BatchV1Api()\n",
    "networking_v1 = client.NetworkingV1Api()\n",
    "# %%\n",
    "# %% [markdown]\n",
    "# Implementing a simple react pattern\n",
    "# %%\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key = os.environ.get(\"NRP_API_KEY\"),\n",
    "    base_url = \"https://llm.nrp-nautilus.io/\"\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Kubernetes assistant.\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentationKnowledgeBase:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.metadata = []\n",
    "        self.api_key = os.environ.get(\"NRP_API_KEY\", \"NRP-API-key-here\")\n",
    "        self.base_url = \"https://llm.nrp-nautilus.io/\"\n",
    "        self.embedding_endpoint = f\"{self.base_url}/v1/embeddings\"\n",
    "        self.rerank_endpoint = f\"{self.base_url}/v1/rerank\"\n",
    "        \n",
    "        # Create a robust session with retries\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\", \"POST\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": \"NRP-Documentation-Crawler/1.0\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        })\n",
    "        \n",
    "    def get_embeddings(self, texts, batch_size=10):\n",
    "        \"\"\"Get embeddings from the NRP API with batching\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process texts in batches to avoid overwhelming the API\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            data = {\n",
    "                \"model\": \"embed-mistral\",\n",
    "                \"input\": batch\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = self.session.post(self.embedding_endpoint, json=data, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    all_embeddings.extend([item['embedding'] for item in result['data']])\n",
    "                    print(f\"Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "                else:\n",
    "                    print(f\"Error getting embeddings: {response.status_code} - {response.text}\")\n",
    "                    # Add zero embeddings as fallback\n",
    "                    all_embeddings.extend([[0.0] * 768] * len(batch))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception when getting embeddings: {e}\")\n",
    "                # Add zero embeddings as fallback\n",
    "                all_embeddings.extend([[0.0] * 768] * len(batch))\n",
    "                \n",
    "            # Add delay between batches\n",
    "            time.sleep(1)\n",
    "            \n",
    "        return all_embeddings\n",
    "    \n",
    "    def rerank_results(self, query, documents, top_k=5):\n",
    "        \"\"\"Rerank search results using the NRP API\"\"\"\n",
    "        # Prepare documents for reranking\n",
    "        docs_for_rerank = [{\"text\": doc['text']} for doc in documents]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"gemma3\",\n",
    "            \"query\": query,\n",
    "            \"documents\": docs_for_rerank,\n",
    "            \"top_n\": top_k\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Reduced timeout to prevent hanging\n",
    "            response = self.session.post(self.rerank_endpoint, json=data, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                # Get indices of top results\n",
    "                top_indices = [item['index'] for item in result['results']]\n",
    "                return [documents[i] for i in top_indices]\n",
    "            else:\n",
    "                print(f\"Error reranking results: {response.status_code} - {response.text}\")\n",
    "                return documents[:top_k]  # Fallback to original order\n",
    "        except Exception as e:\n",
    "            print(f\"Exception when reranking: {e}\")\n",
    "            return documents[:top_k]  # Fallback to original order\n",
    "    \n",
    "    def search(self, query, top_k=5, use_reranking=True):\n",
    "        \"\"\"Search the knowledge base\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            print(\"Knowledge base not loaded. Please load it first.\")\n",
    "            return []\n",
    "            \n",
    "        # Get query embedding\n",
    "        query_embedding = self.get_embeddings([query])\n",
    "        if query_embedding is None:\n",
    "            print(\"Failed to generate query embedding\")\n",
    "            return []\n",
    "            \n",
    "        query_embedding = np.array(query_embedding[0]).reshape(1, -1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings).flatten()\n",
    "        \n",
    "        # Get top results\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': self.documents[idx]['text'],\n",
    "                'url': self.metadata[idx]['url'],\n",
    "                'title': self.metadata[idx]['title'],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "        \n",
    "        # Apply reranking if requested and not disabled\n",
    "        if use_reranking and len(results) > 0:\n",
    "            try:\n",
    "                results = self.rerank_results(query, results, top_k)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Reranking interrupted. Using original results.\")\n",
    "                # Return original results if reranking is interrupted\n",
    "                pass\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def load_knowledge_base(self, filepath):\n",
    "        \"\"\"Load the knowledge base from disk\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        self.documents = data['documents']\n",
    "        self.embeddings = np.array(data['embeddings'])\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        print(f\"Knowledge base loaded from {filepath} with {len(self.documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Prometheus Monitoring Functions\n",
    "# %%\n",
    "def describe_pods(namespace=None):\n",
    "    \"\"\"\n",
    "    Describe pods and print only fields useful for Prometheus metric queries.\n",
    "    If namespace is None, use current namespace. If namespace is \"all\", list all pods.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if namespace == \"all\":\n",
    "            pods = v1.list_pod_for_all_namespaces()\n",
    "        elif namespace is None:\n",
    "            namespace = get_namespace()\n",
    "            pods = v1.list_namespaced_pod(namespace=namespace)\n",
    "        else:\n",
    "            pods = v1.list_namespaced_pod(namespace=namespace)\n",
    "            \n",
    "        rows = []\n",
    "        for pod in pods.items:\n",
    "            pod_name = pod.metadata.name\n",
    "            ns = pod.metadata.namespace\n",
    "            pod_ip = pod.status.pod_ip\n",
    "            node = pod.spec.node_name\n",
    "            container_names = [c.name for c in pod.spec.containers]\n",
    "            container = \", \".join(container_names)\n",
    "            rows.append([pod_name, ns, pod_ip, node, container])\n",
    "        headers = [\"Pod\", \"Namespace\", \"Pod IP\", \"Node\", \"Container\"]\n",
    "        return tabulate(rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "    except ApiException as e:\n",
    "        return f\"❌ Error fetching pods: {e}\"\n",
    "\n",
    "def namespace_gpu_utilization(prom_url=\"https://prometheus.nrp-nautilus.io\", threshold=0):\n",
    "    \"\"\"\n",
    "    Display average GPU utilization per namespace using PromQL.\n",
    "    Args:\n",
    "        prom_url (str): Base Prometheus URL.\n",
    "        threshold (float): Minimum % utilization to show (filtering).\n",
    "    \"\"\"\n",
    "    query = 'avg by (namespace) (DCGM_FI_DEV_GPU_UTIL)'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            return \"❌ Prometheus query failed.\"\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            return \"✅ Query successful, but no GPU usage data returned.\"\n",
    "        rows = []\n",
    "        for r in results:\n",
    "            ns = r[\"metric\"].get(\"namespace\", \"unknown\")\n",
    "            util = float(r[\"value\"][1])\n",
    "            if util >= threshold:\n",
    "                status = (\n",
    "                    \"🟢 Low\" if util < 40 else\n",
    "                    \"🟡 Moderate\" if util < 70 else\n",
    "                    \"🔴 High\"\n",
    "                )\n",
    "                rows.append([ns, f\"{util:.2f}%\", status])\n",
    "        headers = [\"Namespace\", \"Avg GPU Utilization\", \"Status\"]\n",
    "        return tabulate(rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error querying Prometheus: {e}\"\n",
    "\n",
    "def fetch_dcgm_gpu_util_data(prom_url=\"https://prometheus.nrp-nautilus.io\"):\n",
    "    \"\"\"\n",
    "    Fetch rich GPU utilization data from Prometheus using DCGM_FI_DEV_GPU_UTIL.\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts with context: [{hostname, gpu_id, model, namespace, pod, utilization, ...}]\n",
    "    \"\"\"\n",
    "    query = 'DCGM_FI_DEV_GPU_UTIL'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            print(\"❌ Prometheus query failed.\")\n",
    "            return []\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            print(\"✅ Query successful, but no GPU data returned.\")\n",
    "            return []\n",
    "        enriched = []\n",
    "        for r in results:\n",
    "            m = r[\"metric\"]\n",
    "            val = float(r[\"value\"][1])\n",
    "            enriched.append({\n",
    "                \"hostname\": m.get(\"Hostname\", \"unknown\"),\n",
    "                \"ip_port\": m.get(\"instance\", \"unknown\"),\n",
    "                \"gpu_id\": m.get(\"gpu\", \"N/A\"),\n",
    "                \"device\": m.get(\"device\", \"N/A\"),\n",
    "                \"uuid\": m.get(\"UUID\", \"N/A\"),\n",
    "                \"model\": m.get(\"modelName\", \"unknown\"),\n",
    "                \"namespace\": m.get(\"namespace\", \"N/A\"),\n",
    "                \"pod\": m.get(\"pod\", \"N/A\"),\n",
    "                \"utilization\": val\n",
    "            })\n",
    "        return enriched\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying Prometheus: {e}\")\n",
    "        return []\n",
    "\n",
    "def display_gpu_data_head(data, n=5):\n",
    "    \"\"\"\n",
    "    Display the first `n` GPU entries with rich context.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return \"No data to display.\"\n",
    "    rows = [\n",
    "        [d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]]\n",
    "        for d in data[:n]\n",
    "    ]\n",
    "    return tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"fancy_grid\")\n",
    "\n",
    "def analyze_dcgm_gpu_data(data):\n",
    "    \"\"\"\n",
    "    Analyze DCGM GPU data with statistics and top utilization.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return \"No data to analyze.\"\n",
    "    total = len(data)\n",
    "    avg_util = sum(d[\"utilization\"] for d in data) / total\n",
    "    maxed = [d for d in data if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in data if d[\"utilization\"] < 1.0]\n",
    "    available = [d for d in data if d[\"utilization\"] < 100.0]\n",
    "    unique_hosts = set(d[\"hostname\"] for d in data)\n",
    "    unique_models = set(d[\"model\"] for d in data)\n",
    "    \n",
    "    result = f\"\"\"\n",
    "🔍 Total GPUs: {total}\n",
    "📊 Average Utilization: {avg_util:.2f}%\n",
    "🔴 Fully Utilized GPUs (>=99%): {len(maxed)}\n",
    "🟢 Idle GPUs (<1%): {len(idle)}\n",
    "💻 Unique Host Machines: {len(unique_hosts)}\n",
    "🧠 Unique GPU Models: {len(unique_models)}\n",
    "🧮 GPUs Available (<100%): {len(available)}\n",
    "\n",
    "📈 Top 10 GPUs by Utilization:\n",
    "\"\"\"\n",
    "    \n",
    "    top = sorted(data, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [[d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]] for d in top]\n",
    "    result += tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"github\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_gpu_utilization_details(top_n=10, threshold=0):\n",
    "    \"\"\"\n",
    "    Get detailed GPU utilization data.\n",
    "    Args:\n",
    "        top_n: Number of top GPUs to display\n",
    "        threshold: Minimum utilization threshold to include\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    if not filtered:\n",
    "        return f\"✅ No GPUs over {threshold}% utilization.\"\n",
    "    top = sorted(filtered, key=lambda x: x[\"utilization\"], reverse=True)[:top_n]\n",
    "    rows = [\n",
    "        [d[\"hostname\"][:20], d[\"gpu_id\"], d[\"model\"][:25], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"][:20]]\n",
    "        for d in top\n",
    "    ]\n",
    "    result = tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Util%\", \"Namespace\", \"Pod\"], tablefmt=\"grid\")\n",
    "    result += f\"\\n\\nShowing top {len(top)} of {len(filtered)} GPUs above {threshold}% threshold.\"\n",
    "    return result\n",
    "\n",
    "def get_gpu_utilization_stats(threshold=0):\n",
    "    \"\"\"\n",
    "    Get statistical breakdown of GPU utilization.\n",
    "    Args:\n",
    "        threshold: Minimum utilization threshold to include\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    total = len(filtered)\n",
    "    if total == 0:\n",
    "        return f\"✅ No GPUs over the threshold of {threshold}% utilization.\"\n",
    "    avg_util = sum(d[\"utilization\"] for d in filtered) / total\n",
    "    maxed = [d for d in filtered if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in filtered if d[\"utilization\"] < 1.0]\n",
    "    moderate = [d for d in filtered if 1.0 <= d[\"utilization\"] < 70.0]\n",
    "    available = [d for d in filtered if d[\"utilization\"] < 100.0]\n",
    "    unique_models = set(d[\"model\"] for d in filtered)\n",
    "    unique_hosts = set(d[\"hostname\"] for d in filtered)\n",
    "    \n",
    "    return f\"\"\"📊 GPU Utilization Stats (threshold: {threshold}%):\n",
    "🔍 Total GPUs: {total}\n",
    "📈 Average Utilization: {avg_util:.2f}%\n",
    "🔴 Fully Utilized (>=99%): {len(maxed)}\n",
    "🟢 Idle (<1%): {len(idle)}\n",
    "⚙️ Moderate (1-70%): {len(moderate)}\n",
    "💻 Unique Hosts: {len(unique_hosts)}\n",
    "🧠 Unique Models: {len(unique_models)}\n",
    "🧮 Available (<100%): {len(available)}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Initialize Documentation Knowledge Base\n",
    "# %%\n",
    "# Initialize the knowledge base\n",
    "doc_kb = DocumentationKnowledgeBase()\n",
    "# Try to load the knowledge base from file\n",
    "kb_file = \"nrp_expert_docs_kb.json\"\n",
    "if os.path.exists(kb_file):\n",
    "    doc_kb.load_knowledge_base(kb_file)\n",
    "else:\n",
    "    print(f\"❌ Knowledge base file {kb_file} not found. Please provide a pre-built knowledge base file.\")\n",
    "# %% [markdown]\n",
    "# ## Documentation Search Function\n",
    "# %%\n",
    "def search_documentation(query, skip_rerank=False):\n",
    "    \"\"\"\n",
    "    Search the NRP.ai documentation for the given query.\n",
    "    Returns a formatted string with the top results.\n",
    "    Args:\n",
    "        query: The search query\n",
    "        skip_rerank: If True, skip reranking to avoid potential timeouts\n",
    "    \"\"\"\n",
    "    if doc_kb.embeddings is None:\n",
    "        return \"❌ Knowledge base not loaded. Cannot search documentation.\"\n",
    "    \n",
    "    try:\n",
    "        results = doc_kb.search(query, top_k=3, use_reranking=not skip_rerank)\n",
    "        if not results:\n",
    "            return \"❌ No relevant documentation found.\"\n",
    "        \n",
    "        output = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            output.append(f\"Result {i}:\")\n",
    "            output.append(f\"Title: {result['title']}\")\n",
    "            output.append(f\"URL: {result['url']}\")\n",
    "            output.append(f\"Content: {result['text'][:200]}...\")\n",
    "            output.append(\"\")  # Empty line\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "    except KeyboardInterrupt:\n",
    "        return \"❌ Search was interrupted. Please try again.\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during search: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "                        model=\"gemma3\", \n",
    "                        temperature=0,\n",
    "                        messages=self.messages)\n",
    "        return completion.choices[0].message.content\n",
    "# %%\n",
    "prompt = \"\"\"\n",
    "You are a helpful Kubernetes assistant with access to NRP.ai documentation and Prometheus monitoring. Your goal is to be conversational, ask clarifying questions, and guide users to the right resources.\n",
    "\n",
    "**Interaction Style:**\n",
    "- Be conversational and helpful, not robotic\n",
    "- Ask clarifying questions when needed\n",
    "- Guide users through processes step by step\n",
    "- Avoid repetitive answers\n",
    "- Provide context and explanations\n",
    "\n",
    "**Your Approach:**\n",
    "1. When users ask for something that requires more details (like workspace changes, storage increases, etc.), first ask clarifying questions\n",
    "2. Only search documentation after understanding the full context\n",
    "3. Provide clear guidance on next steps\n",
    "4. If a request is outside your capabilities, explain why and direct users to the appropriate resources\n",
    "\n",
    "**Available Actions:**\n",
    "set_namespace:\n",
    "e.g. set_namespace: kube-system\n",
    "Sets the namespace for all operations.\n",
    "list_pods:\n",
    "list_deployments:\n",
    "list_services:\n",
    "list_jobs:\n",
    "list_configmaps:\n",
    "list_secrets:\n",
    "list_pvcs:\n",
    "list_replicasets:\n",
    "list_statefulsets:\n",
    "list_daemonsets:\n",
    "list_events:\n",
    "list_ingresses:\n",
    "list_nodes:\n",
    "Each of the above lists the corresponding resources.\n",
    "describe_pod:\n",
    "describe_deployment:\n",
    "describe_job:\n",
    "describe_service:\n",
    "describe_configmap:\n",
    "describe_secret:\n",
    "describe_pvc:\n",
    "describe_replicaset:\n",
    "describe_statefulset:\n",
    "describe_daemonset:\n",
    "describe_ingress:\n",
    "describe_node:\n",
    "Each of the above describes the specified resource.\n",
    "describe_pods:\n",
    "e.g. describe_pods: gsoc or describe_pods: all\n",
    "Describes pods in a specific namespace or across all namespaces.\n",
    "namespace_gpu_utilization:\n",
    "e.g. namespace_gpu_utilization: 10\n",
    "Shows average GPU utilization per namespace with optional threshold filter.\n",
    "get_gpu_utilization_details:\n",
    "e.g. get_gpu_utilization_details: 10, 20\n",
    "Shows detailed GPU metrics for top 10 GPUs above 20% utilization.\n",
    "get_gpu_utilization_stats:\n",
    "e.g. get_gpu_utilization_stats: 5\n",
    "Shows statistical breakdown of GPU usage above 5% utilization.\n",
    "search_documentation:\n",
    "e.g. search_documentation: How to set up GPU pods in Kubernetes?\n",
    "Searches the NRP.ai documentation for relevant information.\n",
    "\n",
    "**Example Interaction:**\n",
    "User: could I get my Coder workspace (named \"hls4ml-tutorial\") increased to 50 GB please? My email is apk66@drexel.edu\n",
    "Assistant: I'd be happy to help you with increasing your Coder workspace storage. To better assist you, could you please provide a few more details?\n",
    "1. Are you referring to the Coder workspace on the NRP platform?\n",
    "2. Have you already checked the current storage allocation for your workspace?\n",
    "3. Is this for a specific project or workload that requires the additional storage?\n",
    "Once I have these details, I can guide you to the right process for requesting a storage increase.\n",
    "\n",
    "User: Yes, it's on the NRP platform. I need more storage for my machine learning project that's running out of space.\n",
    "Assistant: Thank you for those details. For NRP Coder workspace storage increases, you'll need to submit a request through the official channels. Let me check the documentation for the exact process.\n",
    "Action: search_documentation: How to request Coder workspace storage increase\n",
    "PAUSE\n",
    "(Observation: Result 1:\n",
    "Title: Managing Coder Workspaces\n",
    "URL: https://nrp.ai/documentation/userdocs/coder-workspaces\n",
    "Content: To request changes to your Coder workspace (e.g., storage increase, resource allocation), please submit a support ticket through the NRP Helpdesk at https://help.nrp.ai. Include your workspace name, email address, and the details of your request.\n",
    ")\n",
    "Assistant: Based on the documentation, to increase your Coder workspace storage to 50GB, you'll need to:\n",
    "1. Submit a support ticket through the NRP Helpdesk at https://help.nrp.ai\n",
    "2. Include these details in your request:\n",
    "   - Workspace name: \"hls4ml-tutorial\"\n",
    "   - Your email: apk66@drexel.edu\n",
    "   - Request details: Storage increase to 50GB for machine learning project\n",
    "3. The support team will review your request and make the necessary changes\n",
    "\n",
    "Is there anything else you'd like to know about this process or any other way I can assist you today?\n",
    "\"\"\"\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_NAMESPACE = None\n",
    "def set_namespace(ns):\n",
    "    \"\"\"\n",
    "    Set the global namespace for all Kubernetes operations.\n",
    "    \"\"\"\n",
    "    global CURRENT_NAMESPACE\n",
    "    CURRENT_NAMESPACE = ns.strip()\n",
    "    return f\"✅ Namespace set to '{CURRENT_NAMESPACE}'\"\n",
    "def get_namespace():\n",
    "    \"\"\"\n",
    "    Retrieve the currently set namespace.\n",
    "    Raises an error if namespace is not set.\n",
    "    \"\"\"\n",
    "    if CURRENT_NAMESPACE is None:\n",
    "        raise ValueError(\"❌ Namespace not set. Use `set_namespace` first.\")\n",
    "    return CURRENT_NAMESPACE\n",
    "\n",
    "# %%\n",
    "import re\n",
    "from kubernetes.client.rest import ApiException\n",
    "def validate_k8s_name(name):\n",
    "    \"\"\"\n",
    "    Validate that the name follows Kubernetes RFC1123 naming convention.\n",
    "    \"\"\"\n",
    "    pattern = r'^[a-z0-9]([-a-z0-9]*[a-z0-9])?$'\n",
    "    if not re.match(pattern, name):\n",
    "        raise ValueError(f\"❌ Invalid Kubernetes resource name: '{name}'. Must match RFC1123 format.\")\n",
    "    return name\n",
    "\n",
    "# ---------- LIST FUNCTIONS ----------\n",
    "def list_pods(_=None):\n",
    "    namespace = get_namespace()\n",
    "    pods = v1.list_namespaced_pod(namespace=namespace)\n",
    "    return [pod.metadata.name for pod in pods.items]\n",
    "def list_deployments(_=None):\n",
    "    namespace = get_namespace()\n",
    "    deployments = apps_v1.list_namespaced_deployment(namespace=namespace)\n",
    "    return [d.metadata.name for d in deployments.items]\n",
    "def list_services(_=None):\n",
    "    namespace = get_namespace()\n",
    "    services = v1.list_namespaced_service(namespace=namespace)\n",
    "    return [s.metadata.name for s in services.items]\n",
    "def list_jobs(_=None):\n",
    "    namespace = get_namespace()\n",
    "    jobs = batch_v1.list_namespaced_job(namespace=namespace)\n",
    "    return [j.metadata.name for j in jobs.items]\n",
    "def list_configmaps(_=None):\n",
    "    namespace = get_namespace()\n",
    "    cms = v1.list_namespaced_config_map(namespace=namespace)\n",
    "    return [cm.metadata.name for cm in cms.items]\n",
    "def list_secrets(_=None):\n",
    "    namespace = get_namespace()\n",
    "    secrets = v1.list_namespaced_secret(namespace=namespace)\n",
    "    return [s.metadata.name for s in secrets.items]\n",
    "def list_pvcs(_=None):\n",
    "    namespace = get_namespace()\n",
    "    pvcs = v1.list_namespaced_persistent_volume_claim(namespace=namespace)\n",
    "    return [p.metadata.name for p in pvcs.items]\n",
    "def list_replicasets(_=None):\n",
    "    namespace = get_namespace()\n",
    "    rsets = apps_v1.list_namespaced_replica_set(namespace=namespace)\n",
    "    return [r.metadata.name for r in rsets.items]\n",
    "def list_statefulsets(_=None):\n",
    "    namespace = get_namespace()\n",
    "    ssets = apps_v1.list_namespaced_stateful_set(namespace=namespace)\n",
    "    return [s.metadata.name for s in ssets.items]\n",
    "def list_daemonsets(_=None):\n",
    "    namespace = get_namespace()\n",
    "    dsets = apps_v1.list_namespaced_daemon_set(namespace=namespace)\n",
    "    return [d.metadata.name for d in dsets.items]\n",
    "def list_ingresses(_=None):\n",
    "    namespace = get_namespace()\n",
    "    ingresses = networking_v1.list_namespaced_ingress(namespace=namespace)\n",
    "    return [i.metadata.name for i in ingresses.items]\n",
    "def list_events(_=None):\n",
    "    namespace = get_namespace()\n",
    "    events = v1.list_namespaced_event(namespace=namespace)\n",
    "    return [f\"{e.last_timestamp}: {e.message}\" for e in events.items]\n",
    "def list_nodes(_=None):\n",
    "    nodes = v1.list_node()\n",
    "    return [n.metadata.name for n in nodes.items]\n",
    "\n",
    "# ---------- DESCRIBE FUNCTIONS ----------\n",
    "def describe_pod(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        pod = v1.read_namespaced_pod(name=name, namespace=namespace)\n",
    "        return f\"📋 Pod '{name}' phase: {pod.status.phase}, node: {pod.spec.node_name}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Pod '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise  # Re-raise other exceptions\n",
    "def describe_deployment(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        dep = apps_v1.read_namespaced_deployment(name=name, namespace=namespace)\n",
    "        return f\"📦 Deployment '{name}' has {dep.status.replicas or 0} replicas and {dep.status.ready_replicas or 0} ready.\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Deployment '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_service(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        svc = v1.read_namespaced_service(name=name, namespace=namespace)\n",
    "        return f\"🌐 Service '{name}' type: {svc.spec.type}, cluster IP: {svc.spec.cluster_ip}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Service '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_job(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        job = batch_v1.read_namespaced_job(name=name, namespace=namespace)\n",
    "        return f\"⚙️ Job '{name}' completions: {job.status.succeeded or 0}, active: {job.status.active or 0}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Job '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_configmap(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        cm = v1.read_namespaced_config_map(name=name, namespace=namespace)\n",
    "        keys = list(cm.data.keys()) if cm.data else []\n",
    "        return f\"🗂️ ConfigMap '{name}' has keys: {keys}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ ConfigMap '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_secret(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        sec = v1.read_namespaced_secret(name=name, namespace=namespace)\n",
    "        keys = list(sec.data.keys()) if sec.data else []\n",
    "        return f\"🔒 Secret '{name}' contains {len(keys)} keys (values hidden)\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Secret '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_pvc(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        pvc = v1.read_namespaced_persistent_volume_claim(name=name, namespace=namespace)\n",
    "        return f\"💾 PVC '{name}' status: {pvc.status.phase}, capacity: {pvc.status.capacity.get('storage')}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ PVC '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_replicaset(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        rs = apps_v1.read_namespaced_replica_set(name=name, namespace=namespace)\n",
    "        return f\"📎 ReplicaSet '{name}' replicas: {rs.status.replicas}, ready: {rs.status.ready_replicas}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ ReplicaSet '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_statefulset(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        ss = apps_v1.read_namespaced_stateful_set(name=name, namespace=namespace)\n",
    "        return f\"📘 StatefulSet '{name}' replicas: {ss.status.replicas}, ready: {ss.status.ready_replicas}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ StatefulSet '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_daemonset(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        ds = apps_v1.read_namespaced_daemon_set(name=name, namespace=namespace)\n",
    "        return f\"🔁 DaemonSet '{name}' scheduled: {ds.status.current_number_scheduled}, ready: {ds.status.number_ready}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ DaemonSet '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_ingress(name):\n",
    "    name = name.strip()\n",
    "    namespace = get_namespace()\n",
    "    try:\n",
    "        ing = networking_v1.read_namespaced_ingress(name=name, namespace=namespace)\n",
    "        hosts = [rule.host for rule in ing.spec.rules] if ing.spec.rules else []\n",
    "        services = []\n",
    "        for rule in ing.spec.rules or []:\n",
    "            if rule.http:\n",
    "                for path in rule.http.paths:\n",
    "                    if path.backend and path.backend.service:\n",
    "                        services.append(path.backend.service.name)\n",
    "        return f\"🚪 Ingress '{name}' exposes hosts: {hosts or '[]'} and forwards to services: {services or '[]'}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Ingress '{name}' not found in namespace '{namespace}'.\"\n",
    "        raise\n",
    "def describe_node(name):\n",
    "    name = name.strip()\n",
    "    try:\n",
    "        node = v1.read_node(name=name)\n",
    "        return f\"🖥️ Node '{name}' labels: {node.metadata.labels}\"\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            return f\"❌ Node '{name}' not found.\"\n",
    "        raise\n",
    "\n",
    "# %%\n",
    "known_actions = {\n",
    "    # Namespace control\n",
    "    \"set_namespace\": set_namespace,\n",
    "    # LIST actions\n",
    "    \"list_pods\": list_pods,\n",
    "    \"list_deployments\": list_deployments,\n",
    "    \"list_services\": list_services,\n",
    "    \"list_jobs\": list_jobs,\n",
    "    \"list_configmaps\": list_configmaps,\n",
    "    \"list_secrets\": list_secrets,\n",
    "    \"list_pvcs\": list_pvcs,\n",
    "    \"list_replicasets\": list_replicasets,\n",
    "    \"list_statefulsets\": list_statefulsets,\n",
    "    \"list_daemonsets\": list_daemonsets,\n",
    "    \"list_ingresses\": list_ingresses,\n",
    "    \"list_events\": list_events,\n",
    "    \"list_nodes\": list_nodes,\n",
    "    # DESCRIBE actions\n",
    "    \"describe_pod\": describe_pod,\n",
    "    \"describe_deployment\": describe_deployment,\n",
    "    \"describe_service\": describe_service,\n",
    "    \"describe_job\": describe_job,\n",
    "    \"describe_configmap\": describe_configmap,\n",
    "    \"describe_secret\": describe_secret,\n",
    "    \"describe_pvc\": describe_pvc,\n",
    "    \"describe_replicaset\": describe_replicaset,\n",
    "    \"describe_statefulset\": describe_statefulset,\n",
    "    \"describe_daemonset\": describe_daemonset,\n",
    "    \"describe_ingress\": describe_ingress,\n",
    "    \"describe_node\": describe_node,\n",
    "    \"describe_pods\": describe_pods,\n",
    "    # Prometheus monitoring actions\n",
    "    \"namespace_gpu_utilization\": namespace_gpu_utilization,\n",
    "    \"get_gpu_utilization_details\": get_gpu_utilization_details,\n",
    "    \"get_gpu_utilization_stats\": get_gpu_utilization_stats,\n",
    "    # Documentation search\n",
    "    \"search_documentation\": search_documentation,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# %% [markdown]\n",
    "# # Documentation Search Integration with Prometheus Monitoring\n",
    "# %% [markdown]\n",
    "# This section provides documentation search capabilities and Prometheus monitoring\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Documentation Knowledge Base Class\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "# %% [markdown]\n",
    "# ## Add Loop\n",
    "# %%\n",
    "action_re = re.compile(r'^Action: (\\w+):(.*)$')\n",
    "# %%\n",
    "# %%\n",
    "def query(question, max_turns=15):\n",
    "    i = 0\n",
    "    bot = Agent(prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        i += 1\n",
    "        print(\"Prompt to bot:\", next_prompt)\n",
    "        result = bot(next_prompt)\n",
    "        print(\"Bot response:\\n\", result)\n",
    "        actions = [\n",
    "            action_re.match(a)\n",
    "            for a in result.split('\\n')\n",
    "            if action_re.match(a)\n",
    "        ]\n",
    "        if actions:\n",
    "            for action_match in actions:\n",
    "                action, action_input = action_match.groups()\n",
    "                if action not in known_actions:\n",
    "                    raise Exception(f\"Unknown action: {action}: {action_input}\")\n",
    "                print(f\" -- Running action '{action}' with input '{action_input}'\")\n",
    "                \n",
    "                # Special handling for actions with multiple parameters\n",
    "                if action in [\"get_gpu_utilization_details\"]:\n",
    "                    try:\n",
    "                        # Split parameters by comma\n",
    "                        params = [p.strip() for p in action_input.split(',')]\n",
    "                        top_n = int(params[0]) if len(params) > 0 else 10\n",
    "                        threshold = float(params[1]) if len(params) > 1 else 0\n",
    "                        observation = known_actions[action](top_n=top_n, threshold=threshold)\n",
    "                    except Exception as e:\n",
    "                        observation = f\"❌ Error parsing parameters: {str(e)}\"\n",
    "                elif action in [\"get_gpu_utilization_stats\", \"namespace_gpu_utilization\"]:\n",
    "                    try:\n",
    "                        threshold = float(action_input.strip()) if action_input.strip() else 0\n",
    "                        observation = known_actions[action](threshold=threshold)\n",
    "                    except Exception as e:\n",
    "                        observation = f\"❌ Error parsing parameter: {str(e)}\"\n",
    "                elif action == \"search_documentation\":\n",
    "                    # Add option to skip reranking for search_documentation to avoid timeouts\n",
    "                    try:\n",
    "                        # Check if skip_rerank parameter is provided\n",
    "                        if \",\" in action_input:\n",
    "                            query_text, skip_rerank = action_input.split(\",\", 1)\n",
    "                            skip_rerank = skip_rerank.strip().lower() == \"true\"\n",
    "                        else:\n",
    "                            query_text = action_input\n",
    "                            skip_rerank = False\n",
    "                        observation = known_actions[action](query_text.strip(), skip_rerank=skip_rerank)\n",
    "                    except Exception as e:\n",
    "                        observation = f\"❌ Error during search: {str(e)}\"\n",
    "                else:\n",
    "                    observation = known_actions[action](action_input)\n",
    "                    \n",
    "                print(\"Observation:\", observation)\n",
    "                next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            print(\"No more actions. Halting.\")\n",
    "            return\n",
    "# %%\n",
    "# %% [markdown]\n",
    "# ## Test Documentation Search\n",
    "# %%\n",
    "# Create a new agent with the updated prompt\n",
    "abot = Agent(prompt)\n",
    "# Test the documentation search\n",
    "question = \"How do I configure persistent storage in Kubernetes?\"\n",
    "result = abot(question)\n",
    "print(result)\n",
    "# %%\n",
    "# %%\n",
    "question = \"\"\"I want to see everything related to the name ubuntu in gsoc namespace and explain them\"\"\"\n",
    "query(question)\n",
    "# %%\n",
    "# Test Prometheus monitoring\n",
    "question = \"\"\"Show me GPU utilization across all namespaces\"\"\"\n",
    "query(question)\n",
    "# %%\n",
    "# Test GPU details\n",
    "question = \"\"\"Show me the top 5 most utilized GPUs\"\"\"\n",
    "query(question)\n",
    "# %%\n",
    "# Test GPU statistics\n",
    "question = \"\"\"Give me overall GPU statistics for the cluster\"\"\"\n",
    "query(question)\n",
    "# %%\n",
    "# Test describe pods in all namespaces\n",
    "question = \"\"\"Show me all pods across all namespaces\"\"\"\n",
    "query(question)\n",
    "# %%\n",
    "# Test the new conversational approach for workspace storage request\n",
    "question = \"\"\"could I get my Coder workspace (named \"hls4ml-tutorial\") increased to 50 GB please? My email is apk66@drexel.edu\"\"\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Please run the namespace_gpu_utilization action with threshold 0 to show GPU utilization across all namespaces.\"\"\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "question = \"\"\"We are having issues with pods loading on hcc-nrp-shor-c6029.unl.edu in the central region.\n",
    "For specific details we are seeing 'ErrImagePull'. We are pointing it to an image from Gitlab.\n",
    "in the oulib and oulib-test namespaces.  \"\"\"\n",
    "query(question)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"For NRP Admins, are we still allowed to setup integrations from Gitlab? Following these directions in namespace sdsu-llm. sa=gitlab, secret=gitlab-secret. The instructions do leave out the fact that you need to manually edit the sa to give it access to its secret, but I did that. Gitlab is saying that \"There was a problem authenticating with your cluster. Please ensure your CA Certificate and Token are valid.\" Using the new HA IP of \"https://67.58.53.148:443\". Not sure what I am missing\"\"\"\n",
    "\n",
    "query(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
