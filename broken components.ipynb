{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412705db",
   "metadata": {},
   "source": [
    "LangGraph Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.exceptions import ApiException\n",
    "\n",
    "os.environ[\"NRP_API_KEY\"] = \"API key here\"\n",
    "config.load_incluster_config()\n",
    "\n",
    "v1 = client.CoreV1Api()\n",
    "apps_v1 = client.AppsV1Api()\n",
    "batch_v1 = client.BatchV1Api()\n",
    "networking_v1 = client.NetworkingV1Api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6748b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8a433d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"NRP_API_KEY\"),\n",
    "    base_url=\"https://llm.nrp-nautilus.io/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "47da1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_pods(namespace=\"gsoc\"):\n",
    "    \"\"\"\n",
    "    Describe pods and print only fields useful for Prometheus metric queries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pods = v1.list_namespaced_pod(namespace=namespace) if namespace else v1.list_pod_for_all_namespaces()\n",
    "\n",
    "        rows = []\n",
    "        for pod in pods.items:\n",
    "            pod_name = pod.metadata.name\n",
    "            ns = pod.metadata.namespace\n",
    "            pod_ip = pod.status.pod_ip\n",
    "            node = pod.spec.node_name\n",
    "            container_names = [c.name for c in pod.spec.containers]\n",
    "            container = \", \".join(container_names)\n",
    "\n",
    "            rows.append([pod_name, ns, pod_ip, node, container])\n",
    "\n",
    "        headers = [\"Pod\", \"Namespace\", \"Pod IP\", \"Node\", \"Container\"]\n",
    "        print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"❌ Error fetching pods: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5a21f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════════════════════════╤═════════════╤════════════════╤════════════════════════════╤═════════════╕\n",
      "│ Pod                              │ Namespace   │ Pod IP         │ Node                       │ Container   │\n",
      "╞══════════════════════════════════╪═════════════╪════════════════╪════════════════════════════╪═════════════╡\n",
      "│ agno-deployment-55c55964db-lzhkx │ gsoc        │ 10.244.215.212 │ hcc-nrp-shor-c6013.unl.edu │ jupyter     │\n",
      "├──────────────────────────────────┼─────────────┼────────────────┼────────────────────────────┼─────────────┤\n",
      "│ my-postgres-cluster-0            │ gsoc        │ 10.244.91.149  │ k8s-gen4-02.ampath.net     │ postgres    │\n",
      "├──────────────────────────────────┼─────────────┼────────────────┼────────────────────────────┼─────────────┤\n",
      "│ shellshock-cluster-0             │ gsoc        │ 10.244.19.231  │ dtn-gpu2.kreonet.net       │ postgres    │\n",
      "╘══════════════════════════════════╧═════════════╧════════════════╧════════════════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "describe_pods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1c6d7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#namespace gpu utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f4be1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def namespace_gpu_utilization(prom_url=\"https://prometheus.nrp-nautilus.io\", threshold=0):\n",
    "    \"\"\"\n",
    "    Display average GPU utilization per namespace using PromQL.\n",
    "    Args:\n",
    "        prom_url (str): Base Prometheus URL.\n",
    "        threshold (float): Minimum % utilization to show (filtering).\n",
    "    \"\"\"\n",
    "    query = 'avg by (namespace) (DCGM_FI_DEV_GPU_UTIL)'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            print(\"❌ Prometheus query failed.\")\n",
    "            return\n",
    "\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            print(\"✅ Query successful, but no GPU usage data returned.\")\n",
    "            return\n",
    "\n",
    "        rows = []\n",
    "        for r in results:\n",
    "            ns = r[\"metric\"].get(\"namespace\", \"unknown\")\n",
    "            util = float(r[\"value\"][1])\n",
    "            if util >= threshold:\n",
    "                status = (\n",
    "                    \"🟢 Low\" if util < 40 else\n",
    "                    \"🟡 Moderate\" if util < 70 else\n",
    "                    \"🔴 High\"\n",
    "                )\n",
    "                rows.append([ns, f\"{util:.2f}%\", status])\n",
    "\n",
    "        headers = [\"Namespace\", \"Avg GPU Utilization\", \"Status\"]\n",
    "        print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying Prometheus: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "66825ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════════════════════════╤═══════════════════════╤═════════════╕\n",
      "│ Namespace                          │ Avg GPU Utilization   │ Status      │\n",
      "╞════════════════════════════════════╪═══════════════════════╪═════════════╡\n",
      "│ gpu-mon                            │ 0.14%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csusb-hpc                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nrp-llm                            │ 7.15%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csusb-xli                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-goldberg                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ spatiotemporal-decision-making     │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-rci-jh                        │ 7.31%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-shen-climate-lab              │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsb-cms-ml                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ jupyterlab                         │ 13.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsc-llm                           │ 47.62%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-vigir-gpu               │ 13.43%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ rse-kube                           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ espm-157                           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ehf                                │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ coder                              │ 23.47%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ razvanlab                          │ 20.40%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-llm                           │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aiea-auditors                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ chei-ml                            │ 58.60%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-aicenter                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ seelab                             │ 46.88%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nsf-reu                            │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csusb-chaseci                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ lemn-lab                           │ 19.80%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ omniverse                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ksu-nrp-cluster                    │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ cms-ml                             │ 9.10%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ system-test                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-sknnh                   │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ hjepa                              │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aiea-slugbotics                    │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ wenglab-interpretable-ai           │ 17.20%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ecepxie                            │ 37.80%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aiea-interns                       │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ aoi-lab-scratch                    │ 90.44%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ai-tutoring                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ librareome                         │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ svcl-srip25-clip                   │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsd-ravigroup                     │ 46.75%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ cogrob                             │ 33.89%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-mizzou-xu                │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nourish-sdsc                       │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csun-deep-learning                 │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ urcs-vista                         │ 100.00%               │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ genai-lab                          │ 92.86%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ chronic-opioid-lab                 │ 42.86%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ anthony-lab                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gilpin-lab                         │ 27.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ai-schmidt                         │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csu-tide-jupyterhub                │ 31.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-unoselab02               │ 55.50%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ sdsu-smile                         │ 30.75%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ engr131spring                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ environmental-analytics-group-usra │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ jkb-lab                            │ 98.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ nsf-maica                          │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ isaac-sim                          │ 52.00%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-mizzou-hpdi-pretrain     │ 84.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ niddk                              │ 99.69%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsd-haosulab                      │ 46.97%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ evl                                │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ rohanm                             │ 55.00%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ assel                              │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ai-fusion-ga                       │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csuf-it-test                       │ 89.72%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ svcl-srip25-ad                     │ 81.40%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ erl-ucsd                           │ 25.50%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ dataset-distillation               │ 48.50%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-bml                     │ 21.12%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ svcl-clip                          │ 46.50%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ kc-ai-research-lab                 │ 99.38%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-hpc                     │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ cal-poly-humboldt-microglia        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ amnh-astro-jfagin                  │ 30.00%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ csuf-research                      │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ z-lab                              │ 100.00%               │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gsoc                               │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ williamli-scvl                     │ 31.80%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gp-engine-malof                    │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ece-tarajavidi                     │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ osg-ligo                           │ 20.87%                │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ ucsb-opus-lab                      │ 80.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ vlsida                             │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ pa-riemann                         │ 75.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ osg-icecube                        │ 43.40%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ continual-open-world-learning-lab  │ 93.00%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ mizzou                             │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gpn-mizzou-sgs                     │ 93.67%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ amnh-herpetology-jhoffman1         │ 84.50%                │ 🔴 High     │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ shanxiaojun                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ dl4nlpspace                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ compression                        │ 0.00%                 │ 🟢 Low      │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ gai-lina-group                     │ 57.25%                │ 🟡 Moderate │\n",
      "├────────────────────────────────────┼───────────────────────┼─────────────┤\n",
      "│ biocore-build                      │ 0.00%                 │ 🟢 Low      │\n",
      "╘════════════════════════════════════╧═══════════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "namespace_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "8b8d4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════════════╤═══════╤═════════════════════════╤═══════════════╤═══════════════╤═══════════════════════════════════════════╕\n",
      "│ Host                            │   GPU │ Model                   │ Utilization   │ Namespace     │ Pod                                       │\n",
      "╞═════════════════════════════════╪═══════╪═════════════════════════╪═══════════════╪═══════════════╪═══════════════════════════════════════════╡\n",
      "│ k8s-gpu-01.calit2.optiputer.net │     7 │ NVIDIA GeForce GTX 1080 │ 0.00%         │ gpu-mon       │ dcgm-export-dcgm-exporter-jfh5r           │\n",
      "├─────────────────────────────────┼───────┼─────────────────────────┼───────────────┼───────────────┼───────────────────────────────────────────┤\n",
      "│ rci-tide-gpu-07.sdsu.edu        │     2 │ NVIDIA L40              │ 0.00%         │ csusb-xli     │ jupyter-xiangyu-li-csusb-edu---662a57cb   │\n",
      "├─────────────────────────────────┼───────┼─────────────────────────┼───────────────┼───────────────┼───────────────────────────────────────────┤\n",
      "│ rci-tide-gpu-07.sdsu.edu        │     3 │ NVIDIA L40              │ 0.00%         │ csusb-xli     │ jupyter-xiangyu-li-csusb-edu---662a57cb   │\n",
      "├─────────────────────────────────┼───────┼─────────────────────────┼───────────────┼───────────────┼───────────────────────────────────────────┤\n",
      "│ rci-tide-gpu-01.sdsu.edu        │     0 │ NVIDIA L40              │ 0.00%         │ nrp-llm       │ h2ogpt-sd3-vllm-inference-f48cdb8fd-9pkbb │\n",
      "├─────────────────────────────────┼───────┼─────────────────────────┼───────────────┼───────────────┼───────────────────────────────────────────┤\n",
      "│ rci-tide-gpu-01.sdsu.edu        │     3 │ NVIDIA L40              │ 0.00%         │ sdsu-goldberg │ rag-llm-ollama-7d8d7449d8-z7sg5           │\n",
      "╘═════════════════════════════════╧═══════╧═════════════════════════╧═══════════════╧═══════════════╧═══════════════════════════════════════════╛\n",
      "\n",
      "🔍 Total GPUs: 1327\n",
      "📊 Average Utilization: 23.27%\n",
      "🔴 Fully Utilized GPUs (>=99%): 184\n",
      "🟢 Idle GPUs (<1%): 960\n",
      "💻 Unique Host Machines: 226\n",
      "🧠 Unique GPU Models: 25\n",
      "🧮 GPUs Available (<100%): 1191\n",
      "\n",
      "📈 Top 10 GPUs by Utilization:\n",
      "| Host                        |   GPU | Model                 | Utilization   | Namespace   | Pod                                           |\n",
      "|-----------------------------|-------|-----------------------|---------------|-------------|-----------------------------------------------|\n",
      "| node-1-1.sdsc.optiputer.net |     4 | NVIDIA A100-SXM4-80GB | 100.00%       | jupyterlab  | jupyter-fshaik8-40uic-2eedu                   |\n",
      "| gpu00.nrp.hpc.udel.edu      |     0 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu00.nrp.hpc.udel.edu      |     1 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu00.nrp.hpc.udel.edu      |     2 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu00.nrp.hpc.udel.edu      |     3 | NVIDIA RTX A6000      | 100.00%       | seelab      | jez022-a6000-deployment-pvc3-599d9499fc-wgkk6 |\n",
      "| gpu-09.nrp.mghpcc.org       |     4 | NVIDIA A10            | 100.00%       | ecepxie     | gx-pt-dll4q                                   |\n",
      "| discover-nrp-01.sdccd.edu   |     0 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n",
      "| discover-nrp-01.sdccd.edu   |     1 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n",
      "| discover-nrp-01.sdccd.edu   |     6 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n",
      "| discover-nrp-01.sdccd.edu   |     7 | NVIDIA RTX A6000      | 100.00%       | urcs-vista  | tinyllama-jf-4c-3-6d87cd4fb-svr9n             |\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tabulate import tabulate\n",
    "\n",
    "def fetch_dcgm_gpu_util_data(prom_url=\"https://prometheus.nrp-nautilus.io\"):\n",
    "    \"\"\"\n",
    "    Fetch rich GPU utilization data from Prometheus using DCGM_FI_DEV_GPU_UTIL.\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts with context: [{hostname, gpu_id, model, namespace, pod, utilization, ...}]\n",
    "    \"\"\"\n",
    "    query = 'DCGM_FI_DEV_GPU_UTIL'\n",
    "    url = f\"{prom_url}/api/v1/query\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            print(\"❌ Prometheus query failed.\")\n",
    "            return []\n",
    "\n",
    "        results = data[\"data\"][\"result\"]\n",
    "        if not results:\n",
    "            print(\"✅ Query successful, but no GPU data returned.\")\n",
    "            return []\n",
    "\n",
    "        enriched = []\n",
    "        for r in results:\n",
    "            m = r[\"metric\"]\n",
    "            val = float(r[\"value\"][1])\n",
    "            enriched.append({\n",
    "                \"hostname\": m.get(\"Hostname\", \"unknown\"),\n",
    "                \"ip_port\": m.get(\"instance\", \"unknown\"),\n",
    "                \"gpu_id\": m.get(\"gpu\", \"N/A\"),\n",
    "                \"device\": m.get(\"device\", \"N/A\"),\n",
    "                \"uuid\": m.get(\"UUID\", \"N/A\"),\n",
    "                \"model\": m.get(\"modelName\", \"unknown\"),\n",
    "                \"namespace\": m.get(\"namespace\", \"N/A\"),\n",
    "                \"pod\": m.get(\"pod\", \"N/A\"),\n",
    "                \"utilization\": val\n",
    "            })\n",
    "\n",
    "        return enriched\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying Prometheus: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def display_gpu_data_head(data, n=5):\n",
    "    \"\"\"\n",
    "    Display the first `n` GPU entries with rich context.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    rows = [\n",
    "        [d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]]\n",
    "        for d in data[:n]\n",
    "    ]\n",
    "    print(tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "def analyze_dcgm_gpu_data(data):\n",
    "    \"\"\"\n",
    "    Analyze DCGM GPU data with statistics and top utilization.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to analyze.\")\n",
    "        return\n",
    "\n",
    "    total = len(data)\n",
    "    avg_util = sum(d[\"utilization\"] for d in data) / total\n",
    "    maxed = [d for d in data if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in data if d[\"utilization\"] < 1.0]\n",
    "    available = [d for d in data if d[\"utilization\"] < 100.0]\n",
    "    unique_hosts = set(d[\"hostname\"] for d in data)\n",
    "    unique_models = set(d[\"model\"] for d in data)\n",
    "\n",
    "    print(f\"\\n🔍 Total GPUs: {total}\")\n",
    "    print(f\"📊 Average Utilization: {avg_util:.2f}%\")\n",
    "    print(f\"🔴 Fully Utilized GPUs (>=99%): {len(maxed)}\")\n",
    "    print(f\"🟢 Idle GPUs (<1%): {len(idle)}\")\n",
    "    print(f\"💻 Unique Host Machines: {len(unique_hosts)}\")\n",
    "    print(f\"🧠 Unique GPU Models: {len(unique_models)}\")\n",
    "    print(f\"🧮 GPUs Available (<100%): {len(available)}\\n\")\n",
    "\n",
    "    print(\"📈 Top 10 GPUs by Utilization:\")\n",
    "    top = sorted(data, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [[d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]] for d in top]\n",
    "    print(tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"github\"))\n",
    "\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    display_gpu_data_head(data, n=5)\n",
    "    analyze_dcgm_gpu_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6480981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate_dcgm_gpu_stats(threshold: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Analyze GPU utilization across nodes and return statistical breakdown.\n",
    "    Includes averages, idle/overloaded counts, and model/host distribution.\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    total = len(filtered)\n",
    "    if total == 0:\n",
    "        return f\"✅ No GPUs over the threshold of {threshold}% utilization.\"\n",
    "\n",
    "    avg_util = sum(d[\"utilization\"] for d in filtered) / total\n",
    "    maxed = [d for d in filtered if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in filtered if d[\"utilization\"] < 1.0]\n",
    "    moderate = [d for d in filtered if 1.0 <= d[\"utilization\"] < 70.0]\n",
    "    available = [d for d in filtered if d[\"utilization\"] < 100.0]\n",
    "    unique_models = set(d[\"model\"] for d in filtered)\n",
    "    unique_hosts = set(d[\"hostname\"] for d in filtered)\n",
    "\n",
    "    return f\"\"\"\n",
    "📊 GPU Utilization Stats (threshold: {threshold}%):\n",
    "\n",
    "🔍 Total GPUs Considered: {total}\n",
    "📈 Average Utilization: {avg_util:.2f}%\n",
    "🔴 Fully Utilized (>=99%): {len(maxed)}\n",
    "🟢 Idle (<1%): {len(idle)}\n",
    "⚙️  Moderate (1-70%): {len(moderate)}\n",
    "💻 Unique Host Machines: {len(unique_hosts)}\n",
    "🧠 Unique GPU Models: {len(unique_models)}\n",
    "🧮 GPUs Available (<100%): {len(available)}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "30dbcd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def dcgm_gpu_inspect_tool(threshold: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Inspect raw GPU usage with model name, host, pod, and utilization.\n",
    "    Filters by a minimum utilization threshold.\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    if not filtered:\n",
    "        return f\"✅ No GPUs over {threshold}% utilization.\"\n",
    "\n",
    "    top = sorted(filtered, key=lambda x: x[\"utilization\"], reverse=True)[:10]\n",
    "    rows = [\n",
    "        [d[\"hostname\"], d[\"gpu_id\"], d[\"model\"], f\"{d['utilization']:.2f}%\", d[\"namespace\"], d[\"pod\"]]\n",
    "        for d in top\n",
    "    ]\n",
    "    return tabulate(rows, headers=[\"Host\", \"GPU\", \"Model\", \"Utilization\", \"Namespace\", \"Pod\"], tablefmt=\"github\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0a04c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Optional\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Utility to capture printed output from functions\n",
    "def capture_stdout(func, *args, **kwargs):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    try:\n",
    "        func(*args, **kwargs)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return mystdout.getvalue()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "7c496a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "import json\n",
    "\n",
    "class NRPModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.tools = []\n",
    "\n",
    "    def bind_tools(self, tools):\n",
    "        self.tools = tools\n",
    "        return self\n",
    "\n",
    "    def _convert_tool_to_openai_format(self, tool):\n",
    "        \"\"\"Convert LangChain tool to OpenAI tool format\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.args_schema.model_json_schema() if tool.args_schema else {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def invoke(self, messages):\n",
    "        # Convert messages to proper format if needed\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            if hasattr(msg, 'content'):\n",
    "                role = \"system\" if msg.__class__.__name__ == \"SystemMessage\" else \"user\"\n",
    "                formatted_messages.append({\"role\": role, \"content\": msg.content})\n",
    "            else:\n",
    "                formatted_messages.append(msg)\n",
    "\n",
    "        # Convert tools to OpenAI format\n",
    "        openai_tools = None\n",
    "        if self.tools:\n",
    "            openai_tools = [self._convert_tool_to_openai_format(t) for t in self.tools]\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gemma3\",\n",
    "            temperature=0,\n",
    "            messages=formatted_messages,\n",
    "            tool_choice=\"auto\" if openai_tools else None,\n",
    "            tools=openai_tools,\n",
    "        )\n",
    "\n",
    "        choice = response.choices[0].message\n",
    "\n",
    "        tool_calls = []\n",
    "        if hasattr(choice, \"tool_calls\") and choice.tool_calls:\n",
    "            for t in choice.tool_calls:\n",
    "                # Parse the arguments if they're a string\n",
    "                args = t.function.arguments\n",
    "                if isinstance(args, str):\n",
    "                    try:\n",
    "                        args = json.loads(args)\n",
    "                    except json.JSONDecodeError:\n",
    "                        args = {}\n",
    "                \n",
    "                tool_calls.append({\n",
    "                    \"name\": t.function.name,\n",
    "                    \"args\": args,\n",
    "                    \"id\": t.id\n",
    "                })\n",
    "\n",
    "        return AIMessage(\n",
    "            content=choice.content or \"\",\n",
    "            tool_calls=tool_calls\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "034ba055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "fe9a7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system: str = \"\"):\n",
    "        self.system = system\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        self.raw_graph = graph\n",
    "        self.graph = graph.compile()\n",
    "\n",
    "    def exists_action(self, state: AgentState) -> bool:\n",
    "        \"\"\"Check if the last message has tool calls\"\"\"\n",
    "        try:\n",
    "            result = state[\"messages\"][-1]\n",
    "            \n",
    "            # Check if result has tool_calls attribute and it's not None and not empty\n",
    "            return (hasattr(result, \"tool_calls\") and \n",
    "                    result.tool_calls is not None and \n",
    "                    len(result.tool_calls) > 0)\n",
    "        except (IndexError, KeyError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    def call_openai(self, state: AgentState) -> dict:\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState) -> dict:\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            tool_name = t[\"name\"]\n",
    "            tool_args = t[\"args\"]\n",
    "            print(f\"Calling tool: {tool_name} with args: {tool_args}\")\n",
    "            if tool_name not in self.tools:\n",
    "                result = \"Tool name not recognized. Please try again.\"\n",
    "            else:\n",
    "                try:\n",
    "                    result = self.tools[tool_name].invoke(tool_args)\n",
    "                except Exception as e:\n",
    "                    result = f\"Tool error: {e}\"\n",
    "            results.append(ToolMessage(tool_call_id=t[\"id\"], name=tool_name, content=str(result)))\n",
    "        print(\"✅ Tool(s) executed. Returning to model.\")\n",
    "        return {\"messages\": results}\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6fab9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Updated system prompt\n",
    "system_prompt = \"\"\"You are a Kubernetes monitoring assistant.\n",
    "\n",
    "Use these tools to answer questions:\n",
    "- 'describe_pods_tool': View pod/container info across namespaces.\n",
    "- 'gpu_util_tool': View average GPU utilization per namespace.\n",
    "- 'dcgm_gpu_inspect_tool': Inspect raw GPU metrics by node, GPU model, and pod.\n",
    "- 'calculate_dcgm_gpu_stats': Return a statistical breakdown of all GPU activity (idle, avg, overloaded, unique models).\n",
    "\n",
    "Only respond using actual tool outputs. Say clearly if no data is found. Never guess.\"\"\"\n",
    "\n",
    "# Rebind all tools\n",
    "model = NRPModel(client)\n",
    "tools = [describe_pods_tool, gpu_util_tool, dcgm_gpu_inspect_tool, calculate_dcgm_gpu_stats]\n",
    "abot = Agent(model=model, tools=tools, system=system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fc24ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_core.tools import tool\n",
    "from typing import Optional\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Helper to capture printed output\n",
    "def capture_stdout(func, *args, **kwargs):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    try:\n",
    "        func(*args, **kwargs)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return mystdout.getvalue()\n",
    "\n",
    "# Define tools using decorator without args\n",
    "@tool\n",
    "def describe_pods_tool(namespace: Optional[str] = \"gsoc\") -> str:\n",
    "    \"\"\"Describe pods in a given Kubernetes namespace. Defaults to 'gsoc'.\"\"\"\n",
    "    return capture_stdout(describe_pods, namespace=namespace)\n",
    "\n",
    "@tool\n",
    "def namespace_gpu_util_tool(threshold: Optional[float] = 0.0) -> str:\n",
    "    \"\"\"Get average GPU utilization per namespace with optional threshold filter.\"\"\"\n",
    "    return capture_stdout(namespace_gpu_utilization, threshold=threshold)\n",
    "\n",
    "@tool\n",
    "def calculate_dcgm_gpu_stats(threshold: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Analyze GPU utilization across nodes and return statistical breakdown.\n",
    "    Includes averages, idle/overloaded counts, and model/host distribution.\n",
    "    \"\"\"\n",
    "    data = fetch_dcgm_gpu_util_data()\n",
    "    if not data:\n",
    "        return \"⚠️ No GPU data available.\"\n",
    "\n",
    "    filtered = [d for d in data if d[\"utilization\"] >= threshold]\n",
    "    total = len(filtered)\n",
    "    if total == 0:\n",
    "        return f\"✅ No GPUs over the threshold of {threshold}% utilization.\"\n",
    "\n",
    "    avg_util = sum(d[\"utilization\"] for d in filtered) / total\n",
    "    maxed = [d for d in filtered if d[\"utilization\"] >= 99.0]\n",
    "    idle = [d for d in filtered if d[\"utilization\"] < 1.0]\n",
    "    moderate = [d for d in filtered if 1.0 <= d[\"utilization\"] < 70.0]\n",
    "    available = [d for d in filtered if d[\"utilization\"] < 100.0]\n",
    "    unique_models = set(d[\"model\"] for d in filtered)\n",
    "    unique_hosts = set(d[\"hostname\"] for d in filtered)\n",
    "\n",
    "    return f\"\"\"\n",
    "📊 GPU Utilization Stats (threshold: {threshold}%):\n",
    "\n",
    "🔍 Total GPUs Considered: {total}\n",
    "📈 Average Utilization: {avg_util:.2f}%\n",
    "🔴 Fully Utilized (>=99%): {len(maxed)}\n",
    "🟢 Idle (<1%): {len(idle)}\n",
    "⚙️  Moderate (1-70%): {len(moderate)}\n",
    "💻 Unique Host Machines: {len(unique_hosts)}\n",
    "🧠 Unique GPU Models: {len(unique_models)}\n",
    "🧮 GPUs Available (<100%): {len(available)}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b2bd3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Updated system prompt\n",
    "system_prompt = \"\"\"You are a Kubernetes monitoring assistant.\n",
    "\n",
    "Use these tools to answer questions:\n",
    "- 'describe_pods_tool': View pod/container info across namespaces.\n",
    "- 'gpu_util_tool': View average GPU utilization per namespace.\n",
    "- 'dcgm_gpu_inspect_tool': Inspect raw GPU metrics by node, GPU model, and pod.\n",
    "- 'calculate_dcgm_gpu_stats': Return a statistical breakdown of all GPU activity (idle, avg, overloaded, unique models).\n",
    "\n",
    "Only respond using actual tool outputs. Say clearly if no data is found. Never guess.\"\"\"\n",
    "\n",
    "# Rebind all tools\n",
    "model = NRPModel(client)\n",
    "tools = [describe_pods_tool, namespace_gpu_util_tool, dcgm_gpu_inspect_tool, calculate_dcgm_gpu_stats]\n",
    "abot = Agent(model=model, tools=tools, system=system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "825105b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool: describe_pods_tool with args: {'namespace': 'gsoc'}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: describe_pods_tool with args: {'namespace': 'gsoc'}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: describe_pods_tool with args: {'namespace': 'gsoc'}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: describe_pods_tool with args: {'namespace': 'gsoc'}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "\n",
      "\n",
      "╒══════════════════════════════════╤═════════════╤════════════════╤════════════════════════════╤═════════════╕\n",
      "│ Pod                              │ Namespace   │ Pod IP         │ Node                       │ Container   │\n",
      "╞══════════════════════════════════╪═════════════╪════════════════╪════════════════════════════╪═════════════╡\n",
      "│ agno-deployment-55c55964db-lzhkx │ gsoc        │ 10.244.215.212 │ hcc-nrp-shor-c6013.unl.edu │ jupyter     │\n",
      "├──────────────────────────────────┼─────────────┼────────────────┼────────────────────────────┼─────────────┤\n",
      "│ my-postgres-cluster-0            │ gsoc        │ 10.244.91.149  │ k8s-gen4-02.ampath.net     │ postgres    │\n",
      "├──────────────────────────────────┼─────────────┼────────────────┼────────────────────────────┼─────────────┤\n",
      "│ shellshock-cluster-0             │ gsoc        │ 10.244.19.231  │ dtn-gpu2.kreonet.net       │ postgres    │\n",
      "╘══════════════════════════════════╧═════════════╧════════════════╧════════════════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"List pods in gsoc namespace\")]\n",
    "response = abot.graph.invoke({\"messages\": messages})\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8deb0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: namespace_gpu_util_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[333], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me GPU usage across a few namespaces\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mabot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2844\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[0m\n\u001b[1;32m   2841\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2842\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2844\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2559\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2551\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m   2552\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[1;32m   2558\u001b[0m     )\n\u001b[0;32m-> 2559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[1;32m   2560\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   2561\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Show me GPU usage across a few namespaces\")]\n",
    "response = abot.graph.invoke({\"messages\": messages})\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "8adc5860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n",
      "Calling tool: dcgm_gpu_inspect_tool with args: {'threshold': 0.0}\n",
      "✅ Tool(s) executed. Returning to model.\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[334], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me GPU stats of Hostname=k8s-gpu-03.sdsc.optiputer.net, UUID=GPU-ca8bf369-fe71-b92f-d679-59732bab04e6, container=exporter, device=nvidia7, instance=10.244.20.6:9400, job=dcgm-export-dcgm-exporter, modelName=NVIDIA GeForce GTX 1080 Ti across the cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mabot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2844\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[0m\n\u001b[1;32m   2841\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2842\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2844\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2559\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2551\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m   2552\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[1;32m   2558\u001b[0m     )\n\u001b[0;32m-> 2559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[1;32m   2560\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   2561\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Show me GPU stats of Hostname=k8s-gpu-03.sdsc.optiputer.net, UUID=GPU-ca8bf369-fe71-b92f-d679-59732bab04e6, container=exporter, device=nvidia7, instance=10.244.20.6:9400, job=dcgm-export-dcgm-exporter, modelName=NVIDIA GeForce GTX 1080 Ti across the cluster\")]\n",
    "response = abot.graph.invoke({\"messages\": messages})\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
